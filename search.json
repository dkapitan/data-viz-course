[
  {
    "objectID": "docs/altair_interaction.html",
    "href": "docs/altair_interaction.html",
    "title": "Data visualization course",
    "section": "",
    "text": "“A graphic is not ‘drawn’ once and for all; it is ‘constructed’ and reconstructed until it reveals all the relationships constituted by the interplay of the data. The best graphic operations are those carried out by the decision-maker themself.” — Jacques Bertin\nVisualization provides a powerful means of making sense of data. A single image, however, typically provides answers to, at best, a handful of questions. Through interaction we can transform static images into tools for exploration: highlighting points of interest, zooming in to reveal finer-grained patterns, and linking across multiple views to reason about multi-dimensional relationships.\nAt the core of interaction is the notion of a selection: a means of indicating to the computer which elements or regions we are interested in. For example, we might hover the mouse over a point, click multiple marks, or draw a bounding box around a region to highlight subsets of the data for further scrutiny.\nAlongside visual encodings and data transformations, Altair provides a selection abstraction for authoring interactions. These selections encompass three aspects:\n\nInput event handling to select points or regions of interest, such as mouse hover, click, drag, scroll, and touch events.\nGeneralizing from the input to form a selection rule (or predicate) that determines whether or not a given data record lies within the selection.\nUsing the selection predicate to dynamically configure a visualization by driving conditional encodings, filter transforms, or scale domains.\n\nThis notebook introduces interactive selections and explores how to use them to author a variety of interaction techniques, such as dynamic queries, panning & zooming, details-on-demand, and brushing & linking.\nThis notebook is part of the data visualization curriculum.\n\nimport pandas as pd\nimport altair as alt\n\n\n\nWe will visualize a variety of datasets from the vega-datasets collection:\n\nA dataset of cars from the 1970s and early 1980s,\nA dataset of movies, previously used in the Data Transformation notebook,\nA dataset containing ten years of S&P 500 (sp500) stock prices,\nA dataset of technology company stocks, and\nA dataset of flights, including departure time, distance, and arrival delay.\n\n\ncars = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/cars.json'\nmovies = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'\nsp500 = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/sp500.csv'\nstocks = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/stocks.csv'\nflights = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/flights-5k.json'\n\n\n\n\nLet’s start with a basic selection: simply clicking a point to highlight it. Using the cars dataset, we’ll start with a scatter plot of horsepower versus miles per gallon, with a color encoding for the number cylinders in the car engine.\nIn addition, we’ll create a selection instance by calling alt.selection_single(), indicating we want a selection defined over a single value. By default, the selection uses a mouse click to determine the selected value. To register a selection with a chart, we must add it using the .add_selection() method.\nOnce our selection has been defined, we can use it as a parameter for conditional encodings, which apply a different encoding depending on whether a data record lies in or out of the selection. For example, consider the following code:\ncolor=alt.condition(selection, 'Cylinders:O', alt.value('grey'))\nThis encoding definition states that data points contained within the selection should be colored according to the Cylinder field, while non-selected data points should use a default grey. An empty selection includes all data points, and so initially all points will be colored.\nTry clicking different points in the chart below. What happens? (Click the background to clear the selection state and return to an “empty” selection.)\n\nselection = alt.selection_single();\n  \nalt.Chart(cars).mark_circle().add_selection(\n    selection\n).encode(\n    x='Horsepower:Q',\n    y='Miles_per_Gallon:Q',\n    color=alt.condition(selection, 'Cylinders:O', alt.value('grey')),\n    opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1))\n)\n\n\n\n\n\n\nOf course, highlighting individual data points one-at-a-time is not particularly exciting! As we’ll see, however, single value selections provide a useful building block for more powerful interactions. Moreover, single value selections are just one of the three selection types provided by Altair:\n\nselection_single - select a single discrete value, by default on click events.\nselection_multi - select multiple discrete values. The first value is selected on mouse click and additional values toggled using shift-click.\nselection_interval - select a continuous range of values, initiated by mouse drag.\n\nLet’s compare each of these selection types side-by-side. To keep our code tidy we’ll first define a function (plot) that generates a scatter plot specification just like the one above. We can pass a selection to the plot function to have it applied to the chart:\n\ndef plot(selection):\n    return alt.Chart(cars).mark_circle().add_selection(\n        selection\n    ).encode(\n        x='Horsepower:Q',\n        y='Miles_per_Gallon:Q',\n        color=alt.condition(selection, 'Cylinders:O', alt.value('grey')),\n        opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1))\n    ).properties(\n        width=240,\n        height=180\n    )\n\nLet’s use our plot function to create three chart variants, one per selection type.\nThe first (single) chart replicates our earlier example. The second (multi) chart supports shift-click interactions to toggle inclusion of multiple points within the selection. The third (interval) chart generates a selection region (or brush) upon mouse drag. Once created, you can drag the brush around to select different points, or scroll when the cursor is inside the brush to scale (zoom) the brush size.\nTry interacting with each of the charts below!\n\nalt.hconcat(\n  plot(alt.selection_single()).properties(title='Single (Click)'),\n  plot(alt.selection_multi()).properties(title='Multi (Shift-Click)'),\n  plot(alt.selection_interval()).properties(title='Interval (Drag)')\n)\n\n\n\n\n\n\nThe examples above use default interactions (click, shift-click, drag) for each selection type. We can further customize the interactions by providing input event specifications using Vega event selector syntax. For example, we can modify our single and multi charts to trigger upon mouseover events instead of click events.\nHold down the shift key in the second chart to “paint” with data!\n\nalt.hconcat(\n  plot(alt.selection_single(on='mouseover')).properties(title='Single (Mouseover)'),\n  plot(alt.selection_multi(on='mouseover')).properties(title='Multi (Shift-Mouseover)')\n)\n\n\n\n\n\n\nNow that we’ve covered the basics of Altair selections, let’s take a tour through the various interaction techniques they enable!\n\n\n\nDynamic queries enables rapid, reversible exploration of data to isolate patterns of interest. As defined by Ahlberg, Williamson, & Shneiderman, a dynamic query:\n\nrepresents a query graphically,\nprovides visible limits on the query range,\nprovides a graphical representation of the data and query result,\ngives immediate feedback of the result after every query adjustment,\nand allows novice users to begin working with little training.\n\nA common approach is to manipulate query parameters using standard user interface widgets such as sliders, radio buttons, and drop-down menus. To generate dynamic query widgets, we can apply a selection’s bind operation to one or more data fields we wish to query.\nLet’s build an interactive scatter plot that uses a dynamic query to filter the display. Given a scatter plot of movie ratings (from Rotten Tomates and IMDB), we can add a selection over the Major_Genre field to enable interactive filtering by film genre.\nTo start, let’s extract the unique (non-null) genres from the movies data:\n\ndf = pd.read_json(movies) # load movies data\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\nFor later use, let’s also define a list of unique MPAA_Rating values:\n\nmpaa = ['G', 'PG', 'PG-13', 'R', 'NC-17', 'Not Rated']\n\nNow let’s create a single selection bound to a drop-down menu.\nUse the dynamic query menu below to explore the data. How do ratings vary by genre? How would you revise the code to filter MPAA_Rating (G, PG, PG-13, etc.) instead of Major_Genre?\n\nselectGenre = alt.selection_single(\n    name='Select', # name the selection 'Select'\n    fields=['Major_Genre'], # limit selection to the Major_Genre field\n    init={'Major_Genre': genres[0]}, # use first genre entry as initial value\n    bind=alt.binding_select(options=genres) # bind to a menu of unique genre values\n)\n\nalt.Chart(movies).mark_circle().add_selection(\n    selectGenre\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selectGenre, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\nOur construction above leverages multiple aspects of selections:\n\nWe give the selection a name ('Select'). This name is not required, but allows us to influence the label text of the generated dynamic query menu. (What happens if you remove the name? Try it!)\nWe constrain the selection to a specific data field (Major_Genre). Earlier when we used a single selection, the selection mapped to individual data points. By limiting the selection to a specific field, we can select all data points whose Major_Genre field value matches the single selected value.\nWe initialize init=... the selection to a starting value.\nWe bind the selection to an interface widget, in this case a drop-down menu via binding_select.\nAs before, we then use a conditional encoding to control the opacity channel.\n\n\n\nOne selection instance can be bound to multiple dynamic query widgets. Let’s modify the example above to provide filters for both Major_Genre and MPAA_Rating, using radio buttons instead of a menu. Our single selection is now defined over a single pair of genre and MPAA rating values\nLook for surprising conjunctions of genre and rating. Are there any G or PG-rated horror films?\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(movies).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\nFun facts: The PG-13 rating didn’t exist when the movies Jaws and Jaws 2 were released. The first film to receive a PG-13 rating was 1984’s Red Dawn.\n\n\n\nThough standard interface widgets show the possible query parameter values, they do not visualize the distribution of those values. We might also wish to use richer interactions, such as multi-value or interval selections, rather than input widgets that select only a single value at a time.\nTo address these issues, we can author additional charts to both visualize data and support dynamic queries. Let’s add a histogram of the count of films per year and use an interval selection to dynamically highlight films over selected time periods.\nInteract with the year histogram to explore films from different time periods. Do you seen any evidence of sampling bias across the years? (How do year and critics’ ratings relate?)\nThe years range from 1930 to 2040! Are future films in pre-production, or are there “off-by-one century” errors? Also, depending on which time zone you’re in, you may see a small bump in either 1969 or 1970. Why might that be? (See the end of the notebook for an explanation!)\n\nbrush = alt.selection_interval(\n    encodings=['x'] # limit selection to x-axis (year) values\n)\n\n# dynamic query histogram\nyears = alt.Chart(movies).mark_bar().add_selection(\n    brush\n).encode(\n    alt.X('year(Release_Date):T', title='Films by Release Year'),\n    alt.Y('count():Q', title=None)\n).properties(\n    width=650,\n    height=50\n)\n\n# scatter plot, modify opacity based on selection\nratings = alt.Chart(movies).mark_circle().encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(brush, alt.value(0.75), alt.value(0.05))\n).properties(\n    width=650,\n    height=400\n)\n\nalt.vconcat(years, ratings).properties(spacing=5)\n\n\n\n\n\n\nThe example above provides dynamic queries using a linked selection between charts:\n\nWe create an interval selection (brush), and set encodings=['x'] to limit the selection to the x-axis only, resulting in a one-dimensional selection interval.\nWe register brush with our histogram of films per year via .add_selection(brush).\nWe use brush in a conditional encoding to adjust the scatter plot opacity.\n\nThis interaction technique of selecting elements in one chart and seeing linked highlights in one or more other charts is known as brushing & linking.\n\n\n\n\nThe movie rating scatter plot is a bit cluttered in places, making it hard to examine points in denser regions. Using the interaction techniques of panning and zooming, we can inspect dense regions more closely.\nLet’s start by thinking about how we might express panning and zooming using Altair selections. What defines the “viewport” of a chart? Axis scale domains!\nWe can change the scale domains to modify the visualized range of data values. To do so interactively, we can bind an interval selection to scale domains with the code bind='scales'. The result is that instead of an interval brush that we can drag and zoom, we instead can drag and zoom the entire plotting area!\nIn the chart below, click and drag to pan (translate) the view, or scroll to zoom (scale) the view. What can you discover about the precision of the provided rating values?\n\nalt.Chart(movies).mark_circle().add_selection(\n    alt.selection_interval(bind='scales')\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nZooming in, we can see that the rating values have limited precision! The Rotten Tomatoes ratings are integers, while the IMDB ratings are truncated to tenths. As a result, there is overplotting even when we zoom, with multiple movies sharing the same rating values.\nReading the code above, you may notice the code alt.Axis(minExtent=30) in the y encoding channel. The minExtent parameter ensures a minimum amount of space is reserved for axis ticks and labels. Why do this? When we pan and zoom, the axis labels may change and cause the axis title position to shift. By setting a minimum extent we can reduce distracting movements in the plot. Try changing the minExtent value, for example setting it to zero, and then zoom out to see what happens when longer axis labels enter the view.\nAltair also includes a shorthand for adding panning and zooming to a plot. Instead of directly creating a selection, you can call .interactive() to have Altair automatically generate an interval selection bound to the chart’s scales:\n\nalt.Chart(movies).mark_circle().encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n).interactive()\n\n\n\n\n\n\nBy default, scale bindings for selections include both the x and y encoding channels. What if we want to limit panning and zooming along a single dimension? We can invoke encodings=['x'] to constrain the selection to the x channel only:\n\nalt.Chart(movies).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nWhen zooming along a single axis only, the shape of the visualized data can change, potentially affecting our perception of relationships in the data. Choosing an appropriate aspect ratio is an important visualization design concern!\n\n\n\nWhen panning and zooming, we directly adjust the “viewport” of a chart. The related navigation strategy of overview + detail instead uses an overview display to show all of the data, while supporting selections that pan and zoom a separate focus display.\nBelow we have two area charts showing a decade of price fluctuations for the S&P 500 stock index. Initially both charts show the same data range. Click and drag in the bottom overview chart to update the focus display and examine specific time spans.\n\nbrush = alt.selection_interval(encodings=['x']);\n\nbase = alt.Chart().mark_area().encode(\n    alt.X('date:T', title=None),\n    alt.Y('price:Q')\n).properties(\n    width=700\n)\n  \nalt.vconcat(\n    base.encode(alt.X('date:T', title=None, scale=alt.Scale(domain=brush))),\n    base.add_selection(brush).properties(height=60),\n    data=sp500\n)\n\n\n\n\n\n\nUnlike our earlier panning & zooming case, here we don’t want to bind a selection directly to the scales of a single interactive chart. Instead, we want to bind the selection to a scale domain in another chart. To do so, we update the x encoding channel for our focus chart, setting the scale domain property to reference our brush selection. If no interval is defined (the selection is empty), Altair ignores the brush and uses the underlying data to determine the domain. When a brush interval is created, Altair instead uses that as the scale domain for the focus chart.\n\n\n\nOnce we spot points of interest within a visualization, we often want to know more about them. Details-on-demand refers to interactively querying for more information about selected values. Tooltips are one useful means of providing details on demand. However, tooltips typically only show information for one data point at a time. How might we show more?\nThe movie ratings scatterplot includes a number of potentially interesting outliers where the Rotten Tomatoes and IMDB ratings disagree. Let’s create a plot that allows us to interactively select points and show their labels. To trigger the filter query on either the hover or click interaction, we will use the Altair composition operator | (“or”).\nMouse over points in the scatter plot below to see a highlight and title label. Shift-click points to make annotations persistent and view multiple labels at once. Which movies are loved by Rotten Tomatoes critics, but not the general audience on IMDB (or vice versa)? See if you can find possible errors, where two different movies with the same name were accidentally combined!\n\nhover = alt.selection_single(\n    on='mouseover',  # select on mouseover\n    nearest=True,    # select nearest point to mouse cursor\n    empty='none'     # empty selection should match nothing\n)\n\nclick = alt.selection_multi(\n    empty='none' # empty selection matches no points\n)\n\n# scatter plot encodings shared by all marks\nplot = alt.Chart().mark_circle().encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q'\n)\n  \n# shared base for new layers\nbase = plot.transform_filter(\n    hover | click # filter to points in either selection\n)\n\n# layer scatter plot points, halo annotations, and title labels\nalt.layer(\n    plot.add_selection(hover).add_selection(click),\n    base.mark_point(size=100, stroke='firebrick', strokeWidth=1),\n    base.mark_text(dx=4, dy=-8, align='right', stroke='white', strokeWidth=2).encode(text='Title:N'),\n    base.mark_text(dx=4, dy=-8, align='right').encode(text='Title:N'),\n    data=movies\n).properties(\n    width=600,\n    height=450\n)\n\n\n\n\n\n\nThe example above adds three new layers to the scatter plot: a circular annotation, white text to provide a legible background, and black text showing a film title. In addition, this example uses two selections in tandem:\n\nA single selection (hover) that includes nearest=True to automatically select the nearest data point as the mouse moves.\nA multi selection (click) to create persistent selections via shift-click.\n\nBoth selections include the set empty='none' to indicate that no points should be included if a selection is empty. These selections are then combined into a single filter predicate — the logical or of hover and click — to include points that reside in either selection. We use this predicate to filter the new layers to show annotations and labels for selected points only.\nUsing selections and layers, we can realize a number of different designs for details on demand! For example, here is a log-scaled time series of technology stock prices, annotated with a guideline and labels for the date nearest the mouse cursor:\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=700,\n    height=400\n)\n\n\n\n\n\n\nPutting into action what we’ve learned so far: can you modify the movie scatter plot above (the one with the dynamic query over years) to include a rule mark that shows the average IMDB (or Rotten Tomatoes) rating for the data contained within the year interval selection?\n\n\n\nEarlier in this notebook we saw an example of brushing & linking: using a dynamic query histogram to highlight points in a movie rating scatter plot. Here, we’ll visit some additional examples involving linked selections.\nReturning to the cars dataset, we can use the repeat operator to build a scatter plot matrix (SPLOM) that shows associations between mileage, acceleration, and horsepower. We can define an interval selection and include it within our repeated scatter plot specification to enable linked selections among all the plots.\nClick and drag in any of the plots below to perform brushing & linking!\n\nbrush = alt.selection_interval(\n    resolve='global' # resolve all selections to a single global instance\n)\n\nalt.Chart(cars).mark_circle().add_selection(\n    brush\n).encode(\n    alt.X(alt.repeat('column'), type='quantitative'),\n    alt.Y(alt.repeat('row'), type='quantitative'),\n    color=alt.condition(brush, 'Cylinders:O', alt.value('grey')),\n    opacity=alt.condition(brush, alt.value(0.8), alt.value(0.1))\n).properties(\n    width=140,\n    height=140\n).repeat(\n    column=['Acceleration', 'Horsepower', 'Miles_per_Gallon'],\n    row=['Miles_per_Gallon', 'Horsepower', 'Acceleration']\n)\n\n\n\n\n\n\nNote above the use of resolve='global' on the interval selection. The default setting of 'global' indicates that across all plots only one brush can be active at a time. However, in some cases we might want to define brushes in multiple plots and combine the results. If we use resolve='union', the selection will be the union of all brushes: if a point resides within any brush it will be selected. Alternatively, if we use resolve='intersect', the selection will consist of the intersection of all brushes: only points that reside within all brushes will be selected.\nTry setting the resolve parameter to 'union' and 'intersect' and see how it changes the resulting selection logic.\n\n\nThe brushing & linking examples we’ve looked at all use conditional encodings, for example to change opacity values in response to a selection. Another option is to use a selection defined in one view to filter the content of another view.\nLet’s build a collection of histograms for the flights dataset: arrival delay (how early or late a flight arrives, in minutes), distance flown (in miles), and time of departure (hour of the day). We’ll use the repeat operator to create the histograms, and add an interval selection for the x axis with brushes resolved via intersection.\nIn particular, each histogram will consist of two layers: a gray background layer and a blue foreground layer, with the foreground layer filtered by our intersection of brush selections. The result is a cross-filtering interaction across the three charts!\nDrag out brush intervals in the charts below. As you select flights with longer or shorter arrival delays, how do the distance and time distributions respond?\n\nbrush = alt.selection_interval(\n    encodings=['x'],\n    resolve='intersect'\n);\n\nhist = alt.Chart().mark_bar().encode(\n    alt.X(alt.repeat('row'), type='quantitative',\n        bin=alt.Bin(maxbins=100, minstep=1), # up to 100 bins\n        axis=alt.Axis(format='d', titleAnchor='start') # integer format, left-aligned title\n    ),\n    alt.Y('count():Q', title=None) # no y-axis title\n)\n  \nalt.layer(\n    hist.add_selection(brush).encode(color=alt.value('lightgrey')),\n    hist.transform_filter(brush)\n).properties(\n    width=900,\n    height=100\n).repeat(\n    row=['delay', 'distance', 'time'],\n    data=flights\n).transform_calculate(\n    delay='datum.delay < 180 ? datum.delay : 180', # clamp delays > 3 hours\n    time='hours(datum.date) + minutes(datum.date) / 60' # fractional hours\n).configure_view(\n    stroke='transparent' # no outline\n)\n\n\n\n\n\n\nBy cross-filtering you can observe that delayed flights are more likely to depart at later hours. This phenomenon is familiar to frequent fliers: a delay can propagate through the day, affecting subsequent travel by that plane. For the best odds of an on-time arrival, book an early flight!\nThe combination of multiple views and interactive selections can enable valuable forms of multi-dimensional reasoning, turning even basic histograms into powerful input devices for asking questions of a dataset!\n\n\n\n\nFor more information about the supported interaction options in Altair, please consult the Altair interactive selection documentation. For details about customizing event handlers, for example to compose multiple interaction techniques or support touch-based input on mobile devices, see the Vega-Lite selection documentation.\nInterested in learning more? - The selection abstraction was introduced in the paper Vega-Lite: A Grammar of Interactive Graphics, by Satyanarayan, Moritz, Wongsuphasawat, & Heer. - The PRIM-9 system (for projection, rotation, isolation, and masking in up to 9 dimensions) is one of the earliest interactive visualization tools, built in the early 1970s by Fisherkeller, Tukey, & Friedman. A retro demo video survives! - The concept of brushing & linking was crystallized by Becker, Cleveland, & Wilks in their 1987 article Dynamic Graphics for Data Analysis. - For a comprehensive summary of interaction techniques for visualization, see Interactive Dynamics for Visual Analysis by Heer & Shneiderman. - Finally, for a treatise on what makes interaction effective, read the classic Direct Manipulation Interfaces paper by Hutchins, Hollan, & Norman.\n\n\nEarlier we observed a small bump in the number of movies in either 1969 and 1970. Where does that bump come from? And why 1969 or 1970? The answer stems from a combination of missing data and how your computer represents time.\nInternally, dates and times are represented relative to the UNIX epoch, in which time “zero” corresponds to the stroke of midnight on January 1, 1970 in UTC time, which runs along the prime meridian. It turns out there are a few movies with missing (null) release dates. Those null values get interpreted as time 0, and thus map to January 1, 1970 in UTC time. If you live in the Americas – and thus in “earlier” time zones – this precise point in time corresponds to an earlier hour on December 31, 1969 in your local time zone. On the other hand, if you live near or east of the prime meridian, the date in your local time zone will be January 1, 1970.\nThe takeaway? Always be skeptical of your data, and be mindful that how data is represented (whether as date times, or floating point numbers, or latitudes and longitudes, etc.) can sometimes lead to artifacts that impact analysis!"
  },
  {
    "objectID": "docs/altair_debugging.html",
    "href": "docs/altair_debugging.html",
    "title": "Data visualization course",
    "section": "",
    "text": "In this notebook we show you common debugging techniques that you can use if you run into issues with Altair.\nYou can jump to the following sections:\n\nInstallation and Setup when Altair is not installed correctly\nDisplay Issues when you don’t see a chart\nInvalid Specifications when you get an error\nProperties are Being Ignored when you don’t see any errors or warnings\nAsking for Help when you get stuck\nReporting Issues when you find a bug\n\nIn addition to this notebook, you might find the Frequently Asked Questions and Display Troubleshooting guides helpful.\nThis notebook is part of the data visualization curriculum.\n\n\nThese instructions follow the Altair documentation but focus on some specifics for this series of notebooks.\nIn every notebook, we will import the Altair and Vega Datasets packages. If you are running this notebook on Colab, Altair and Vega Datasets should be preinstalled and ready to go. The notebooks in this series are designed for Colab but should also work in Jupyter Lab or the Jupyter Notebook (the notebook requires a bit more setup described below) but additional packages are required.\nIf you are running in Jupyter Lab or Jupyter Notebooks, you have to install the necessary packages by running the following command in your terminal.\npip install altair vega_datasets\nOr if you use Conda\nconda install -c conda-forge altair vega_datasets\nYou can run command line commands from a code cell by prefixing it with !. For example, to install Altair and Vega Datasets with Pip, you can run the following cell.\n\n!pip install altair vega_datasets\n\nRequirement already satisfied: altair in ./.venv/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: vega_datasets in ./.venv/lib/python3.8/site-packages (0.9.0)\nRequirement already satisfied: jsonschema in ./.venv/lib/python3.8/site-packages (from altair) (3.2.0)\n\n\nRequirement already satisfied: toolz in ./.venv/lib/python3.8/site-packages (from altair) (0.11.1)\nRequirement already satisfied: entrypoints in ./.venv/lib/python3.8/site-packages (from altair) (0.3)\nRequirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from altair) (1.20.3)\nRequirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from altair) (3.0.0)\nRequirement already satisfied: pandas>=0.18 in ./.venv/lib/python3.8/site-packages (from altair) (1.2.4)\nRequirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from jsonschema->altair) (47.1.0)\nRequirement already satisfied: six>=1.11.0 in ./.venv/lib/python3.8/site-packages (from jsonschema->altair) (1.16.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in ./.venv/lib/python3.8/site-packages (from jsonschema->altair) (0.17.3)\nRequirement already satisfied: attrs>=17.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema->altair) (21.2.0)\n\n\nRequirement already satisfied: MarkupSafe>=2.0.0rc2 in ./.venv/lib/python3.8/site-packages (from jinja2->altair) (2.0.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in ./.venv/lib/python3.8/site-packages (from pandas>=0.18->altair) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in ./.venv/lib/python3.8/site-packages (from pandas>=0.18->altair) (2021.1)\n\n\nWARNING: You are using pip version 20.1.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/Users/jjallaire/quarto/demo/visualization-curriculum/.venv/bin/python3 -m pip install --upgrade pip' command.\n\n\n\nimport altair as alt\nfrom vega_datasets import data\n\n\n\nIf you are running into issues with Altair, first make sure that you are running the latest version. To check the version of Altair that you have installed, run the cell below.\n\nalt.__version__\n\n'4.1.0'\n\n\nTo check what the latest version of altair is, go to this page or run the cell below (requires Python 3).\n\nimport urllib.request, json \nwith urllib.request.urlopen(\"https://pypi.org/pypi/altair/json\") as url:\n    print(json.loads(url.read().decode())['info']['version'])\n\n4.1.0\n\n\nIf you are not running the latest version, you can update it with pip. You can update Altair and Vega Datasets by running this command in your terminal.\npip install -U altair vega_datasets\n\n\n\nNow you can create an Altair chart.\n\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Displacement',\n    color='Origin'\n)\n\n\n\n\n\n\n\n\n\nIf you are running in Jupyter Lab, Jupyter Notebook, or Colab (and have a working Internet connection) you should be seeing a chart. If you are running in another environment (or offline), you will need to tell Altair to use a different renderer;\nTo activate a different renderer in a notebook cell:\n# to run in nteract, VSCode, or offline in JupyterLab\nalt.renderers.enable('mimebundle')\nTo run offline in Jupyter Notebook you must install an additional dependency, the vega package. Run this command in your terminal:\npip install vega\nThen activate the notebook renderer:\n# to run offline in Jupyter Notebook\nalt.renderers.enable('notebook')\nThese instruction follow the instructions on the Altair website.\n\n\n\n\nIf you are having issues with seeing a chart, make sure your setup is correct by following the debugging instruction above. If you are still having issues, follow the instruction about debugging display issues in the Altair documentation.\n\n\nA common error is accidentally using a field that does not exit.\n\nimport pandas as pd\n\ndf = pd.DataFrame({'x': [1, 2, 3],\n                     'y': [3, 1, 4]})\n\nalt.Chart(df).mark_point().encode(\n    x='x:Q',\n    y='y:Q',\n    color='color:Q'  # <-- this field does not exist in the data!\n)\n\n\n\n\n\n\nCheck the spelling of your files and print the data source to confirm that the data and fields exist. For instance, here you see that color is not a vaid field.\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1\n      3\n    \n    \n      1\n      2\n      1\n    \n    \n      2\n      3\n      4\n    \n  \n\n\n\n\n\n\n\n\nAnother common issue is creating an invalid specification and getting an error.\n\n\nAltair might show an SchemaValidationError or ValueError. Read the error message carefully. Usually it will tell you what is going wrong.\nFor example, if you forget the mark type, you will see this SchemaValidationError.\n\nalt.Chart(cars).encode(\n    y='Horsepower'\n)\n\nSchemaValidationError: Invalid specification\n\n        altair.vegalite.v4.api.Chart, validating 'required'\n\n        'mark' is a required property\n        \n\n\nalt.Chart(...)\n\n\nOr if you use a non-existent channel, you get a ValueError.\n\nalt.Chart(cars)).mark_point().encode(\n    z='Horsepower'\n)\n\nSyntaxError: unmatched ')' (<ipython-input-9-fc84db126677>, line 1)\n\n\n\n\n\n\nAltair might ignore a property that you specified. In the chart below, we are using a text channel, which is only compatible with mark_text. You do not see an error or a warning about this in the notebook. However, the underlying Vega-Lite library will show a warning in the browser console. Press Alt+Cmd+I on Mac or Alt+Ctrl+I on Windows and Linux to open the developer tools and click on the Console tab. When you run the example in the cell below, you will see a the following warning.\nWARN text dropped as it is incompatible with \"bar\".\n\nalt.Chart(cars).mark_bar().encode(\n    y='mean(Horsepower)',\n    text='mean(Acceleration)'\n)\n\n\n\n\n\n\nIf you find yourself debugging issues related to Vega-Lite, you can open the chart in the Vega Editor either by clicking on the “Open in Vega Editor” link at the bottom of the chart or in the action menu (click to open) at the top right of a chart. The Vega Editor provides additional debugging but you will be writing Vega-Lite JSON instead of Altair in Python.\nNote: The Vega Editor may be using a newer version of Vega-Lite and so the behavior may vary.\n\n\n\nIf you find a problem with Altair and get stuck, you can ask a question on Stack Overflow. Ask your question with the altair and vega-lite tags. You can find a list of questions people have asked before here.\n\n\n\nIf you find a problem with Altair and believe it is a bug, please create an issue in the Altair GitHub repo with a description of your problem. If you believe the issue is related to the underlying Vega-Lite library, please create an issue in the Vega-Lite GitHub repo."
  },
  {
    "objectID": "docs/altair_introduction.html",
    "href": "docs/altair_introduction.html",
    "title": "Data visualization course",
    "section": "",
    "text": "Altair is a declarative statistical visualization library for Python. Altair offers a powerful and concise visualization grammar for quickly building a wide range of statistical graphics.\nBy declarative, we mean that you can provide a high-level specification of what you want the visualization to include, in terms of data, graphical marks, and encoding channels, rather than having to specify how to implement the visualization in terms of for-loops, low-level drawing commands, etc. The key idea is that you declare links between data fields and visual encoding channels, such as the x-axis, y-axis, color, etc. The rest of the plot details are handled automatically. Building on this declarative plotting idea, a surprising range of simple to sophisticated visualizations can be created using a concise grammar.\nAltair is based on Vega-Lite, a high-level grammar of interactive graphics. Altair provides a friendly Python API (Application Programming Interface) that generates Vega-Lite specifications in JSON (JavaScript Object Notation) format. Environments such as Jupyter Notebooks, JupyterLab, and Colab can then take this specification and render it directly in the web browser. To learn more about the motivation and basic concepts behind Altair and Vega-Lite, watch the Vega-Lite presentation video from OpenVisConf 2017.\nThis notebook will guide you through the basic process of creating visualizations in Altair. First, you will need to make sure you have the Altair package and its dependencies installed (for more, see the Altair installation documentation), or you are using a notebook environment that includes the dependencies pre-installed.\n\n\nTo start, we must import the necessary libraries: Pandas for data frames and Altair for visualization.\n\nimport pandas as pd\nimport altair as alt\n\n\n\n\nDepending on your environment, you may need to specify a renderer for Altair. If you are using JupyterLab, Jupyter Notebook, or Google Colab with a live Internet connection you should not need to do anything. Otherwise, please read the documentation for Displaying Altair Charts.\n\n\n\nData in Altair is built around the Pandas data frame, which consists of a set of named data columns. We will also regularly refer to data columns as data fields.\nWhen using Altair, datasets are commonly provided as data frames. Alternatively, Altair can also accept a URL to load a network-accessible dataset. As we will see, the named columns of the data frame are an essential piece of plotting with Altair.\nWe will often use datasets from the vega-datasets repository. Some of these datasets are directly available as Pandas data frames:\n\nfrom vega_datasets import data  # import vega_datasets\ncars = data.cars()              # load cars data as a Pandas data frame\ncars.head()                     # display the first five rows\n\n\n\n\n\n  \n    \n      \n      Name\n      Miles_per_Gallon\n      Cylinders\n      Displacement\n      Horsepower\n      Weight_in_lbs\n      Acceleration\n      Year\n      Origin\n    \n  \n  \n    \n      0\n      chevrolet chevelle malibu\n      18.0\n      8\n      307.0\n      130.0\n      3504\n      12.0\n      1970-01-01\n      USA\n    \n    \n      1\n      buick skylark 320\n      15.0\n      8\n      350.0\n      165.0\n      3693\n      11.5\n      1970-01-01\n      USA\n    \n    \n      2\n      plymouth satellite\n      18.0\n      8\n      318.0\n      150.0\n      3436\n      11.0\n      1970-01-01\n      USA\n    \n    \n      3\n      amc rebel sst\n      16.0\n      8\n      304.0\n      150.0\n      3433\n      12.0\n      1970-01-01\n      USA\n    \n    \n      4\n      ford torino\n      17.0\n      8\n      302.0\n      140.0\n      3449\n      10.5\n      1970-01-01\n      USA\n    \n  \n\n\n\n\nDatasets in the vega-datasets collection can also be accessed via URLs:\n\ndata.cars.url\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/cars.json'\n\n\nDataset URLs can be passed directly to Altair (for supported formats like JSON and CSV), or loaded into a Pandas data frame like so:\n\npd.read_json(data.cars.url).head() # load JSON data into a data frame\n\n\n\n\n\n  \n    \n      \n      Name\n      Miles_per_Gallon\n      Cylinders\n      Displacement\n      Horsepower\n      Weight_in_lbs\n      Acceleration\n      Year\n      Origin\n    \n  \n  \n    \n      0\n      chevrolet chevelle malibu\n      18.0\n      8\n      307.0\n      130.0\n      3504\n      12.0\n      1970-01-01\n      USA\n    \n    \n      1\n      buick skylark 320\n      15.0\n      8\n      350.0\n      165.0\n      3693\n      11.5\n      1970-01-01\n      USA\n    \n    \n      2\n      plymouth satellite\n      18.0\n      8\n      318.0\n      150.0\n      3436\n      11.0\n      1970-01-01\n      USA\n    \n    \n      3\n      amc rebel sst\n      16.0\n      8\n      304.0\n      150.0\n      3433\n      12.0\n      1970-01-01\n      USA\n    \n    \n      4\n      ford torino\n      17.0\n      8\n      302.0\n      140.0\n      3449\n      10.5\n      1970-01-01\n      USA\n    \n  \n\n\n\n\nFor more information about data frames - and some useful transformations to prepare Pandas data frames for plotting with Altair! - see the Specifying Data with Altair documentation.\n\n\nStatistical visualization in Altair begins with “tidy” data frames. Here, we’ll start by creating a simple data frame (df) containing the average precipitation (precip) for a given city and month :\n\ndf = pd.DataFrame({\n    'city': ['Seattle', 'Seattle', 'Seattle', 'New York', 'New York', 'New York', 'Chicago', 'Chicago', 'Chicago'],\n    'month': ['Apr', 'Aug', 'Dec', 'Apr', 'Aug', 'Dec', 'Apr', 'Aug', 'Dec'],\n    'precip': [2.68, 0.87, 5.31, 3.94, 4.13, 3.58, 3.62, 3.98, 2.56]\n})\n\ndf\n\n\n\n\n\n  \n    \n      \n      city\n      month\n      precip\n    \n  \n  \n    \n      0\n      Seattle\n      Apr\n      2.68\n    \n    \n      1\n      Seattle\n      Aug\n      0.87\n    \n    \n      2\n      Seattle\n      Dec\n      5.31\n    \n    \n      3\n      New York\n      Apr\n      3.94\n    \n    \n      4\n      New York\n      Aug\n      4.13\n    \n    \n      5\n      New York\n      Dec\n      3.58\n    \n    \n      6\n      Chicago\n      Apr\n      3.62\n    \n    \n      7\n      Chicago\n      Aug\n      3.98\n    \n    \n      8\n      Chicago\n      Dec\n      2.56\n    \n  \n\n\n\n\n\n\n\n\nThe fundamental object in Altair is the Chart, which takes a data frame as a single argument:\n\nchart = alt.Chart(df)\n\nSo far, we have defined the Chart object and passed it the simple data frame we generated above. We have not yet told the chart to do anything with the data.\n\n\n\nWith a chart object in hand, we can now specify how we would like the data to be visualized. We first indicate what kind of graphical mark (geometric shape) we want to use to represent the data. We can set the mark attribute of the chart object using the the Chart.mark_* methods.\nFor example, we can show the data as a point using Chart.mark_point():\n\nalt.Chart(df).mark_point()\n\n\n\n\n\n\nHere the rendering consists of one point per row in the dataset, all plotted on top of each other, since we have not yet specified positions for these points.\nTo visually separate the points, we can map various encoding channels, or channels for short, to fields in the dataset. For example, we could encode the field city of the data using the y channel, which represents the y-axis position of the points. To specify this, use the encode method:\n\nalt.Chart(df).mark_point().encode(\n  y='city',\n)\n\n\n\n\n\n\nThe encode() method builds a key-value mapping between encoding channels (such as x, y, color, shape, size, etc.) to fields in the dataset, accessed by field name. For Pandas data frames, Altair automatically determines an appropriate data type for the mapped column, which in this case is the nominal type, indicating unordered, categorical values.\nThough we’ve now separated the data by one attribute, we still have multiple points overlapping within each category. Let’s further separate these by adding an x encoding channel, mapped to the 'precip' field:\n\nalt.Chart(df).mark_point().encode(\n    x='precip',\n    y='city'\n)\n\n\n\n\n\n\nSeattle exhibits both the least-rainiest and most-rainiest months!\nThe data type of the 'precip' field is again automatically inferred by Altair, and this time is treated as a quantitative type (that is, a real-valued number). We see that grid lines and appropriate axis titles are automatically added as well.\nAbove we have specified key-value pairs using keyword arguments (x='precip'). In addition, Altair provides construction methods for encoding definitions, using the syntax alt.X('precip'). This alternative is useful for providing more parameters to an encoding, as we will see later in this notebook.\n\nalt.Chart(df).mark_point().encode(\n    alt.X('precip'),\n    alt.Y('city')\n)\n\n\n\n\n\n\nThe two styles of specifying encodings can be interleaved: x='precip', alt.Y('city') is also a valid input to the encode function.\nIn the examples above, the data type for each field is inferred automatically based on its type within the Pandas data frame. We can also explicitly indicate the data type to Altair by annotating the field name:\n\n'b:N' indicates a nominal type (unordered, categorical data),\n'b:O' indicates an ordinal type (rank-ordered data),\n'b:Q' indicates a quantitative type (numerical data with meaningful magnitudes), and\n'b:T' indicates a temporal type (date/time data)\n\nFor example, alt.X('precip:N').\nExplicit annotation of data types is necessary when data is loaded from an external URL directly by Vega-Lite (skipping Pandas entirely), or when we wish to use a type that differs from the type that was automatically inferred.\nWhat do you think will happen to our chart above if we treat precip as a nominal or ordinal variable, rather than a quantitative variable? Modify the code above and find out!\nWe will take a closer look at data types and encoding channels in the next notebook of the data visualization curriculum.\n\n\n\nTo allow for more flexibility in how data are visualized, Altair has a built-in syntax for aggregation of data. For example, we can compute the average of all values by specifying an aggregation function along with the field name:\n\nalt.Chart(df).mark_point().encode(\n    x='average(precip)',\n    y='city'\n)\n\n\n\n\n\n\nNow within each x-axis category, we see a single point reflecting the average of the values within that category.\nDoes Seattle really have the lowest average precipitation of these cities? (It does!) Still, how might this plot mislead? Which months are included? What counts as precipitation?\nAltair supports a variety of aggregation functions, including count, min (minimum), max (maximum), average, median, and stdev (standard deviation). In a later notebook, we will take a tour of data transformations, including aggregation, sorting, filtering, and creation of new derived fields using calculation formulas.\n\n\n\nLet’s say we want to represent our aggregated values using rectangular bars rather than circular points. We can do this by replacing Chart.mark_point with Chart.mark_bar:\n\nalt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city'\n)\n\n\n\n\n\n\nBecause the nominal field a is mapped to the y-axis, the result is a horizontal bar chart. To get a vertical bar chart, we can simply swap the x and y keywords:\n\nalt.Chart(df).mark_bar().encode(\n    x='city',\n    y='average(precip)'\n)\n\n\n\n\n\n\n\n\n\nBy default Altair / Vega-Lite make some choices about properties of the visualization, but these can be changed using methods to customize the look of the visualization. For example, we can specify the axis titles using the axis attribute of channel classes, we can modify scale properties using the scale attribute, and we can specify the color of the marking by setting the color keyword of the Chart.mark_* methods to any valid CSS color string:\n\nalt.Chart(df).mark_point(color='firebrick').encode(\n  alt.X('precip', scale=alt.Scale(type='log'), axis=alt.Axis(title='Log-Scaled Values')),\n  alt.Y('city', axis=alt.Axis(title='Category')),\n)\n\n\n\n\n\n\nA subsequent module will explore the various options available for scales, axes, and legends to create customized charts.\n\n\n\nAs we’ve seen above, the Altair Chart object represents a plot with a single mark type. What about more complicated diagrams, involving multiple charts or layers? Using a set of view composition operators, Altair can take multiple chart definitions and combine them to create more complex views.\nAs a starting point, let’s plot the cars dataset in a line chart showing the average mileage by the year of manufacture:\n\nalt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\n\n\n\n\n\nTo augment this plot, we might like to add circle marks for each averaged data point. (The circle mark is just a convenient shorthand for point marks that used filled circles.)\nWe can start by defining each chart separately: first a line plot, then a scatter plot. We can then use the layer operator to combine the two into a layered chart. Here we use the shorthand + (plus) operator to invoke layering:\n\nline = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\npoint = alt.Chart(cars).mark_circle().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\nline + point\n\n\n\n\n\n\nWe can also create this chart by reusing and modifying a previous chart definition! Rather than completely re-write a chart, we can start with the line chart, then invoke the mark_point method to generate a new chart definition with a different mark type:\n\nmpg = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\nmpg + mpg.mark_circle()\n\n\n\n\n\n\n(The need to place points on lines is so common, the line mark also includes a shorthand to generate a new layer for you. Trying adding the argument point=True to the mark_line method!)\nNow, what if we’d like to see this chart alongside other plots, such as the average horsepower over time?\nWe can use concatenation operators to place multiple charts side-by-side, either vertically or horizontally. Here, we’ll use the | (pipe) operator to perform horizontal concatenation of two charts:\n\nhp = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Horsepower)')\n)\n\n(mpg + mpg.mark_circle()) | (hp + hp.mark_circle())\n\n\n\n\n\n\nWe can see that, in this dataset, over the 1970s and early ’80s the average fuel efficiency improved while the average horsepower decreased.\nA later notebook will focus on view composition, including not only layering and concatenation, but also the facet operator for splitting data into sub-plots and the repeat operator to concisely generate concatenated charts from a template.\n\n\n\nIn addition to basic plotting and view composition, one of Altair and Vega-Lite’s most exciting features is its support for interaction.\nTo create a simple interactive plot that supports panning and zooming, we can invoke the interactive() method of the Chart object. In the chart below, click and drag to pan or use the scroll wheel to zoom:\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n\n\nTo provide more details upon mouse hover, we can use the tooltip encoding channel:\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n    tooltip=['Name', 'Origin'] # show Name and Origin in a tooltip\n).interactive()\n\n\n\n\n\n\nFor more complex interactions, such as linked charts and cross-filtering, Altair provides a selection abstraction for defining interactive selections and then binding them to components of a chart. We will cover this is in detail in a later notebook.\nBelow is a more complex example. The upper histogram shows the count of cars per year and uses an interactive selection to modify the opacity of points in the lower scatter plot, which shows horsepower versus mileage.\nDrag out an interval in the upper chart and see how it affects the points in the lower chart. As you examine the code, don’t worry if parts don’t make sense yet! This is an aspirational example, and we will fill in all the needed details over the course of the different notebooks.\n\n# create an interval selection over an x-axis encoding\nbrush = alt.selection_interval(encodings=['x'])\n\n# determine opacity based on brush\nopacity = alt.condition(brush, alt.value(0.9), alt.value(0.1))\n\n# an overview histogram of cars per year\n# add the interval brush to select cars over time\noverview = alt.Chart(cars).mark_bar().encode(\n    alt.X('Year:O', timeUnit='year', # extract year unit, treat as ordinal\n      axis=alt.Axis(title=None, labelAngle=0) # no title, no label angle\n    ),\n    alt.Y('count()', title=None), # counts, no axis title\n    opacity=opacity\n).add_selection(\n    brush      # add interval brush selection to the chart\n).properties(\n    width=400, # set the chart width to 400 pixels\n    height=50  # set the chart height to 50 pixels\n)\n\n# a detail scatterplot of horsepower vs. mileage\n# modulate point opacity based on the brush selection\ndetail = alt.Chart(cars).mark_point().encode(\n    alt.X('Horsepower'),\n    alt.Y('Miles_per_Gallon'),\n    # set opacity based on brush selection\n    opacity=opacity\n).properties(width=400) # set chart width to match the first chart\n\n# vertically concatenate (vconcat) charts using the '&' operator\noverview & detail\n\n\n\n\n\n\n\n\n\nAs a Python API to Vega-Lite, Altair’s main purpose is to convert plot specifications to a JSON string that conforms to the Vega-Lite schema. Using the Chart.to_json method, we can inspect the JSON specification that Altair is exporting and sending to Vega-Lite:\n\nchart = alt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city',\n)\nprint(chart.to_json())\n\n{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\",\n  \"config\": {\n    \"view\": {\n      \"continuousHeight\": 300,\n      \"continuousWidth\": 400\n    }\n  },\n  \"data\": {\n    \"name\": \"data-fdfbb22e8e0e89f6556d8a3b434b0c97\"\n  },\n  \"datasets\": {\n    \"data-fdfbb22e8e0e89f6556d8a3b434b0c97\": [\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Apr\",\n        \"precip\": 2.68\n      },\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Aug\",\n        \"precip\": 0.87\n      },\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Dec\",\n        \"precip\": 5.31\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Apr\",\n        \"precip\": 3.94\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Aug\",\n        \"precip\": 4.13\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Dec\",\n        \"precip\": 3.58\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Apr\",\n        \"precip\": 3.62\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Aug\",\n        \"precip\": 3.98\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Dec\",\n        \"precip\": 2.56\n      }\n    ]\n  },\n  \"encoding\": {\n    \"x\": {\n      \"aggregate\": \"average\",\n      \"field\": \"precip\",\n      \"type\": \"quantitative\"\n    },\n    \"y\": {\n      \"field\": \"city\",\n      \"type\": \"nominal\"\n    }\n  },\n  \"mark\": \"bar\"\n}\n\n\nNotice here that encode(x='average(precip)') has been expanded to a JSON structure with a field name, a type for the data, and includes an aggregate field. The encode(y='city') statement has been expanded similarly.\nAs we saw earlier, Altair’s shorthand syntax includes a way to specify the type of the field as well:\n\nx = alt.X('average(precip):Q')\nprint(x.to_json())\n\n{\n  \"aggregate\": \"average\",\n  \"field\": \"precip\",\n  \"type\": \"quantitative\"\n}\n\n\nThis short-hand is equivalent to spelling-out the attributes by name:\n\nx = alt.X(aggregate='average', field='precip', type='quantitative')\nprint(x.to_json())\n\n{\n  \"aggregate\": \"average\",\n  \"field\": \"precip\",\n  \"type\": \"quantitative\"\n}\n\n\n\n\n\nOnce you have visualized your data, perhaps you would like to publish it somewhere on the web. This can be done straightforwardly using the vega-embed JavaScript package. A simple example of a stand-alone HTML document can be generated for any chart using the Chart.save method:\nchart = alt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city',\n)\nchart.save('chart.html')\nThe basic HTML template produces output that looks like this, where the JSON specification for your plot produced by Chart.to_json should be stored in the spec JavaScript variable:\n<!DOCTYPE html>\n<html>\n  <head>\n    <script src=\"https://cdn.jsdelivr.net/npm/vega@5\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"></script>\n  </head>\n  <body>\n  <div id=\"vis\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {}; /* JSON output for your chart's specification */\n      var embedOpt = {\"mode\": \"vega-lite\"}; /* Options for the embedding */\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n  </script>\n</body>\n</html>\nThe Chart.save method provides a convenient way to save such HTML output to file. For more information on embedding Altair/Vega-Lite, see the documentation of the vega-embed project.\n\n\n\n🎉 Hooray, you’ve completed the introduction to Altair! In the next notebook, we will dive deeper into creating visualizations using Altair’s model of data types, graphical marks, and visual encoding channels."
  },
  {
    "objectID": "docs/altair_cartographic.html",
    "href": "docs/altair_cartographic.html",
    "title": "Data visualization course",
    "section": "",
    "text": "“The making of maps is one of humanity’s longest established intellectual endeavors and also one of its most complex, with scientific theory, graphical representation, geographical facts, and practical considerations blended together in an unending variety of ways.” — H. J. Steward\nCartography – the study and practice of map-making – has a rich history spanning centuries of discovery and design. Cartographic visualization leverages mapping techniques to convey data containing spatial information, such as locations, routes, or trajectories on the surface of the Earth.\n\n\n\nApproximating the Earth as a sphere, we can denote positions using a spherical coordinate system of latitude (angle in degrees north or south of the equator) and longitude (angle in degrees specifying east-west position). In this system, a parallel is a circle of constant latitude and a meridian is a circle of constant longitude. The prime meridian lies at 0° longitude and by convention is defined to pass through the Royal Observatory in Greenwich, England.\nTo “flatten” a three-dimensional sphere on to a two-dimensional plane, we must apply a projection that maps (longitude, latitude) pairs to (x, y) coordinates. Similar to scales, projections map from a data domain (spatial position) to a visual range (pixel position). However, the scale mappings we’ve seen thus far accept a one-dimensional domain, whereas map projections are inherently two-dimensional.\nIn this notebook, we will introduce the basics of creating maps and visualizing spatial data with Altair, including:\n\nData formats for representing geographic features,\nGeo-visualization techniques such as point, symbol, and choropleth maps, and\nA review of common cartographic projections.\n\nThis notebook is part of the data visualization curriculum.\n\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n\n\nUp to this point, we have worked with JSON and CSV formatted datasets that correspond to data tables made up of rows (records) and columns (fields). In order to represent geographic regions (countries, states, etc.) and trajectories (flight paths, subway lines, etc.), we need to expand our repertoire with additional formats designed to support rich geometries.\nGeoJSON models geographic features within a specialized JSON format. A GeoJSON feature can include geometric data – such as longitude, latitude coordinates that make up a country boundary – as well as additional data attributes.\nHere is a GeoJSON feature object for the boundary of the U.S. state of Colorado:\n{\n  \"type\": \"Feature\",\n  \"id\": 8,\n  \"properties\": {\"name\": \"Colorado\"},\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [[-106.32056285448942,40.998675790862656],[-106.19134826714341,40.99813863734313],[-105.27607827344248,40.99813863734313],[-104.9422739227986,40.99813863734313],[-104.05212898774828,41.00136155846029],[-103.57475287338661,41.00189871197981],[-103.38093099236758,41.00189871197981],[-102.65589358559272,41.00189871197981],[-102.62000064466328,41.00189871197981],[-102.052892177978,41.00189871197981],[-102.052892177978,40.74889940428302],[-102.052892177978,40.69733266640851],[-102.052892177978,40.44003613055551],[-102.052892177978,40.3492571857556],[-102.052892177978,40.00333031918079],[-102.04930288388505,39.57414465707943],[-102.04930288388505,39.56823596836465],[-102.0457135897921,39.1331416175485],[-102.0457135897921,39.0466599009048],[-102.0457135897921,38.69751011321283],[-102.0457135897921,38.61478847120581],[-102.0457135897921,38.268861604631],[-102.0457135897921,38.262415762396685],[-102.04212429569915,37.738153927339205],[-102.04212429569915,37.64415206142214],[-102.04212429569915,37.38900413964724],[-102.04212429569915,36.99365914927603],[-103.00046581851544,37.00010499151034],[-103.08660887674611,37.00010499151034],[-104.00905745863294,36.99580776335414],[-105.15404227428235,36.995270609834606],[-105.2222388620483,36.995270609834606],[-105.7175614468747,36.99580776335414],[-106.00829426840322,36.995270609834606],[-106.47490250048605,36.99365914927603],[-107.4224761410235,37.00010499151034],[-107.48349414060355,37.00010499151034],[-108.38081766383978,36.99903068447129],[-109.04483707103458,36.99903068447129],[-109.04483707103458,37.484617466122884],[-109.04124777694163,37.88049961001363],[-109.04124777694163,38.15283644441336],[-109.05919424740635,38.49983761802722],[-109.05201565922046,39.36680339854235],[-109.05201565922046,39.49786885730673],[-109.05201565922046,39.66062637372313],[-109.05201565922046,40.22248895514744],[-109.05201565922046,40.653823231326896],[-109.05201565922046,41.000287251421234],[-107.91779872584989,41.00189871197981],[-107.3183866123281,41.00297301901887],[-106.85895696843116,41.00189871197981],[-106.32056285448942,40.998675790862656]]\n    ]\n  }\n}\nThe feature includes a properties object, which can include any number of data fields, plus a geometry object, which in this case contains a single polygon that consists of [longitude, latitude] coordinates for the state boundary. The coordinates continue off to the right for a while should you care to scroll…\nTo learn more about the nitty-gritty details of GeoJSON, see the official GeoJSON specification or read Tom MacWright’s helpful primer.\nOne drawback of GeoJSON as a storage format is that it can be redundant, resulting in larger file sizes. Consider: Colorado shares boundaries with six other states (seven if you include the corner touching Arizona). Instead of using separate, overlapping coordinate lists for each of those states, a more compact approach is to encode shared borders only once, representing the topology of geographic regions. Fortunately, this is precisely what the TopoJSON format does!\nLet’s load a TopoJSON file of world countries (at 110 meter resolution):\n\nworld = data.world_110m.url\nworld\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/world-110m.json'\n\n\n\nworld_topo = data.world_110m()\n\n\nworld_topo.keys()\n\ndict_keys(['type', 'transform', 'objects', 'arcs'])\n\n\n\nworld_topo['type']\n\n'Topology'\n\n\n\nworld_topo['objects'].keys()\n\ndict_keys(['land', 'countries'])\n\n\nInspect the world_topo TopoJSON dictionary object above to see its contents.\nIn the data above, the objects property indicates the named elements we can extract from the data: geometries for all countries, or a single polygon representing all land on Earth. Either of these can be unpacked to GeoJSON data we can then visualize.\nAs TopoJSON is a specialized format, we need to instruct Altair to parse the TopoJSON format, indicating which named faeture object we wish to extract from the topology. The following code indicates that we want to extract GeoJSON features from the world dataset for the countries object:\nalt.topo_feature(world, 'countries')\nThis alt.topo_feature method call expands to the following Vega-Lite JSON:\n{\n  \"values\": world,\n  \"format\": {\"type\": \"topojson\", \"feature\": \"countries\"}\n}\nNow that we can load geographic data, we’re ready to start making maps!\n\n\n\nTo visualize geographic data, Altair provides the geoshape mark type. To create a basic map, we can create a geoshape mark and pass it our TopoJSON data, which is then unpacked into GeoJSON features, one for each country of the world:\n\nalt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape()\n\n\n\n\n\n\nIn the example above, Altair applies a default blue color and uses a default map projection (mercator). We can customize the colors and boundary stroke widths using standard mark properties. Using the project method we can also add our own map projection:\n\nalt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape(\n    fill='#2a1d0c', stroke='#706545', strokeWidth=0.5\n).project(\n    type='mercator'\n)\n\n\n\n\n\n\nBy default Altair automatically adjusts the projection so that all the data fits within the width and height of the chart. We can also specify projection parameters, such as scale (zoom level) and translate (panning), to customize the projection settings. Here we adjust the scale and translate parameters to focus on Europe:\n\nalt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape(\n    fill='#2a1d0c', stroke='#706545', strokeWidth=0.5\n).project(\n    type='mercator', scale=400, translate=[100, 550]\n)\n\n\n\n\n\n\nNote how the 110m resolution of the data becomes apparent at this scale. To see more detailed coast lines and boundaries, we need an input file with more fine-grained geometries. Adjust the scale and translate parameters to focus the map on other regions!\nSo far our map shows countries only. Using the layer operator, we can combine multiple map elements. Altair includes data generators we can use to create data for additional map layers:\n\nThe sphere generator ({'sphere': True}) provides a GeoJSON representation of the full sphere of the Earth. We can create an additional geoshape mark that fills in the shape of the Earth as a background layer.\nThe graticule generator ({'graticule': ...}) creates a GeoJSON feature representing a graticule: a grid formed by lines of latitude and longitude. The default graticule has meridians and parallels every 10° between ±80° latitude. For the polar regions, there are meridians every 90°. These settings can be customized using the stepMinor and stepMajor properties.\n\nLet’s layer sphere, graticule, and country marks into a reusable map specification:\n\nmap = alt.layer(\n    # use the sphere of the Earth as the base layer\n    alt.Chart({'sphere': True}).mark_geoshape(\n        fill='#e6f3ff'\n    ),\n    # add a graticule for geographic reference lines\n    alt.Chart({'graticule': True}).mark_geoshape(\n        stroke='#ffffff', strokeWidth=1\n    ),\n    # and then the countries of the world\n    alt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape(\n        fill='#2a1d0c', stroke='#706545', strokeWidth=0.5\n    )\n).properties(\n    width=600,\n    height=400\n)\n\nWe can extend the map with a desired projection and draw the result. Here we apply a Natural Earth projection. The sphere layer provides the light blue background; the graticule layer provides the white geographic reference lines.\n\nmap.project(\n    type='naturalEarth1', scale=110, translate=[300, 200]\n).configure_view(stroke=None)\n\n\n\n\n\n\n\n\n\nIn addition to the geometric data provided by GeoJSON or TopoJSON files, many tabular datasets include geographic information in the form of fields for longitude and latitude coordinates, or references to geographic regions such as country names, state names, postal codes, etc., which can be mapped to coordinates using a geocoding service. In some cases, location data is rich enough that we can see meaningful patterns by projecting the data points alone — no base map required!\nLet’s look at a dataset of 5-digit zip codes in the United States, including longitude, latitude coordinates for each post office in addition to a zip_code field.\n\nzipcodes = data.zipcodes.url\nzipcodes\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/zipcodes.csv'\n\n\nWe can visualize each post office location using a small (1-pixel) square mark. However, to set the positions we do not use x and y channels. Why is that?\nWhile cartographic projections map (longitude, latitude) coordinates to (x, y) coordinates, they can do so in arbitrary ways. There is no guarantee, for example, that longitude → x and latitude → y! Instead, Altair includes special longitude and latitude encoding channels to handle geographic coordinates. These channels indicate which data fields should be mapped to longitude and latitude coordinates, and then applies a projection to map those coordinates to (x, y) positions.\n\nalt.Chart(zipcodes).mark_square(\n    size=1, opacity=1\n).encode(\n    longitude='longitude:Q', # apply the field named 'longitude' to the longitude channel\n    latitude='latitude:Q'    # apply the field named 'latitude' to the latitude channel\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nPlotting zip codes only, we can see the outline of the United States and discern meaningful patterns in the density of post offices, without a base map or additional reference elements!\nWe use the albersUsa projection, which takes some liberties with the actual geometry of the Earth, with scaled versions of Alaska and Hawaii in the bottom-left corner. As we did not specify projection scale or translate parameters, Altair sets them automatically to fit the visualized data.\nWe can now go on to ask more questions of our dataset. For example, is there any rhyme or reason to the allocation of zip codes? To assess this question we can add a color encoding based on the first digit of the zip code. We first add a calculate transform to extract the first digit, and encode the result using the color channel:\n\nalt.Chart(zipcodes).transform_calculate(\n    digit='datum.zip_code[0]'\n).mark_square(\n    size=2, opacity=1\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='digit:N'\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nTo zoom in on a specific digit, add a filter transform to limit the data shown! Try adding an interactive selection to filter to a single digit and dynamically update the map. And be sure to use strings (`‘1’`) instead of numbers (`1`) when filtering digit values!\n(This example is inspired by Ben Fry’s classic zipdecode visualization!)\nWe might further wonder what the sequence of zip codes might indicate. One way to explore this question is to connect each consecutive zip code using a line mark, as done in Robert Kosara’s ZipScribble visualization:\n\nalt.Chart(zipcodes).transform_filter(\n    '-150 < datum.longitude && 22 < datum.latitude && datum.latitude < 55'\n).transform_calculate(\n    digit='datum.zip_code[0]'\n).mark_line(\n    strokeWidth=0.5\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='digit:N',\n    order='zip_code:O'\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nWe can now see how zip codes further cluster into smaller areas, indicating a hierarchical allocation of codes by location, but with some notable variability within local clusters.\nIf you were paying careful attention to our earlier maps, you may have noticed that there are zip codes being plotted in the upper-left corner! These correspond to locations such as Puerto Rico or American Samoa, which contain U.S. zip codes but are mapped to null coordinates (0, 0) by the albersUsa projection. In addition, Alaska and Hawaii can complicate our view of the connecting line segments. In response, the code above includes an additional filter that removes points outside our chosen longitude and latitude spans.\nRemove the filter above to see what happens!\n\n\n\nNow let’s combine a base map and plotted data as separate layers. We’ll examine the U.S. commercial flight network, considering both airports and flight routes. To do so, we’ll need three datasets. For our base map, we’ll use a TopoJSON file for the United States at 10m resolution, containing features for states or counties:\n\nusa = data.us_10m.url\nusa\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/us-10m.json'\n\n\nFor the airports, we will use a dataset with fields for the longitude and latitude coordinates of each airport as well as the iata airport code — for example, 'SEA' for Seattle-Tacoma International Airport.\n\nairports = data.airports.url\nairports\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/airports.csv'\n\n\nFinally, we will use a dataset of flight routes, which contains origin and destination fields with the IATA codes for the corresponding airports:\n\nflights = data.flights_airport.url\nflights\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/flights-airport.csv'\n\n\nLet’s start by creating a base map using the albersUsa projection, and add a layer that plots circle marks for each airport:\n\nalt.layer(\n    alt.Chart(alt.topo_feature(usa, 'states')).mark_geoshape(\n        fill='#ddd', stroke='#fff', strokeWidth=1\n    ),\n    alt.Chart(airports).mark_circle(size=9).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        tooltip='iata:N'\n    )\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nThat’s a lot of airports! Obviously, not all of them are major hubs.\nSimilar to our zip codes dataset, our airport data includes points that lie outside the continental United States. So we again see points in the upper-left corner. We might want to filter these points, but to do so we first need to know more about them.\nUpdate the map projection above to albers – side-stepping the idiosyncratic behavior of albersUsa – so that the actual locations of these additional points is revealed!\nNow, instead of showing all airports in an undifferentiated fashion, let’s identify major hubs by considering the total number of routes that originate at each airport. We’ll use the routes dataset as our primary data source: it contains a list of flight routes that we can aggregate to count the number of routes for each origin airport.\nHowever, the routes dataset does not include the locations of the airports! To augment the routes data with locations, we need a new data transformation: lookup. The lookup transform takes a field value in a primary dataset and uses it as a key to look up related information in another table. In this case, we want to match the origin airport code in our routes dataset against the iata field of the airports dataset, then extract the corresponding latitude and longitude fields.\n\nalt.layer(\n    alt.Chart(alt.topo_feature(usa, 'states')).mark_geoshape(\n        fill='#ddd', stroke='#fff', strokeWidth=1\n    ),\n    alt.Chart(flights).mark_circle().transform_aggregate(\n        groupby=['origin'],\n        routes='count()'\n    ).transform_lookup(\n        lookup='origin',\n        from_=alt.LookupData(data=airports, key='iata',\n                             fields=['state', 'latitude', 'longitude'])\n    ).transform_filter(\n        'datum.state !== \"PR\" && datum.state !== \"VI\"'\n    ).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        tooltip=['origin:N', 'routes:Q'],\n        size=alt.Size('routes:Q', scale=alt.Scale(range=[0, 1000]), legend=None),\n        order=alt.Order('routes:Q', sort='descending')\n    )\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nWhich U.S. airports have the highest number of outgoing routes?\nNow that we can see the airports, which may wish to interact with them to better understand the structure of the air traffic network. We can add a rule mark layer to represent paths from origin airports to destination airports, which requires two lookup transforms to retreive coordinates for each end point. In addition, we can use a single selection to filter these routes, such that only the routes originating at the currently selected airport are shown.\nStarting from the static map above, can you build an interactive version? Feel free to skip the code below to engage with the interactive map first, and think through how you might build it on your own!\n\n# interactive selection for origin airport\n# select nearest airport to mouse cursor\norigin = alt.selection_single(\n    on='mouseover', nearest=True,\n    fields=['origin'], empty='none'\n)\n\n# shared data reference for lookup transforms\nforeign = alt.LookupData(data=airports, key='iata',\n                         fields=['latitude', 'longitude'])\n    \nalt.layer(\n    # base map of the United States\n    alt.Chart(alt.topo_feature(usa, 'states')).mark_geoshape(\n        fill='#ddd', stroke='#fff', strokeWidth=1\n    ),\n    # route lines from selected origin airport to destination airports\n    alt.Chart(flights).mark_rule(\n        color='#000', opacity=0.35\n    ).transform_filter(\n        origin # filter to selected origin only\n    ).transform_lookup(\n        lookup='origin', from_=foreign # origin lat/lon\n    ).transform_lookup(\n        lookup='destination', from_=foreign, as_=['lat2', 'lon2'] # dest lat/lon\n    ).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        latitude2='lat2',\n        longitude2='lon2',\n    ),\n    # size airports by number of outgoing routes\n    # 1. aggregate flights-airport data set\n    # 2. lookup location data from airports data set\n    # 3. remove Puerto Rico (PR) and Virgin Islands (VI)\n    alt.Chart(flights).mark_circle().transform_aggregate(\n        groupby=['origin'],\n        routes='count()'\n    ).transform_lookup(\n        lookup='origin',\n        from_=alt.LookupData(data=airports, key='iata',\n                             fields=['state', 'latitude', 'longitude'])\n    ).transform_filter(\n        'datum.state !== \"PR\" && datum.state !== \"VI\"'\n    ).add_selection(\n        origin\n    ).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        tooltip=['origin:N', 'routes:Q'],\n        size=alt.Size('routes:Q', scale=alt.Scale(range=[0, 1000]), legend=None),\n        order=alt.Order('routes:Q', sort='descending') # place smaller circles on top\n    )\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nMouseover the map to probe the flight network!\n\n\n\nA choropleth map uses shaded or textured regions to visualize data values. Sized symbol maps are often more accurate to read, as people tend to be better at estimating proportional differences between the area of circles than between color shades. Nevertheless, choropleth maps are popular in practice and particularly useful when too many symbols become perceptually overwhelming.\nFor example, while the United States only has 50 states, it has thousands of counties within those states. Let’s build a choropleth map of the unemployment rate per county, back in the recession year of 2008. In some cases, input GeoJSON or TopoJSON files might include statistical data that we can directly visualize. In this case, however, we have two files: our TopoJSON file that includes county boundary features (usa), and a separate text file that contains unemployment statistics:\n\nunemp = data.unemployment.url\nunemp\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/unemployment.tsv'\n\n\nTo integrate our data sources, we will again need to use the lookup transform, augmenting our TopoJSON-based geoshape data with unemployment rates. We can then create a map that includes a color encoding for the looked-up rate field.\n\nalt.Chart(alt.topo_feature(usa, 'counties')).mark_geoshape(\n    stroke='#aaa', strokeWidth=0.25\n).transform_lookup(\n    lookup='id', from_=alt.LookupData(data=unemp, key='id', fields=['rate'])\n).encode(\n    alt.Color('rate:Q',\n              scale=alt.Scale(domain=[0, 0.3], clamp=True), \n              legend=alt.Legend(format='%')),\n    alt.Tooltip('rate:Q', format='.0%')\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nExamine the unemployment rates by county. Higher values in Michigan may relate to the automotive industry. Counties in the Great Plains and Mountain states exhibit both low and high rates. Is this variation meaningful, or is it possibly an artifact of lower sample sizes? To explore further, try changing the upper scale domain (e.g., to 0.2) to adjust the color mapping.\nA central concern for choropleth maps is the choice of colors. Above, we use Altair’s default 'yellowgreenblue' scheme for heatmaps. Below we compare other schemes, including a single-hue sequential scheme (teals) that varies in luminance only, a multi-hue sequential scheme (viridis) that ramps in both luminance and hue, and a diverging scheme (blueorange) that uses a white mid-point:\n\n# utility function to generate a map specification for a provided color scheme\ndef map_(scheme):\n    return alt.Chart().mark_geoshape().project(type='albersUsa').encode(\n        alt.Color('rate:Q', scale=alt.Scale(scheme=scheme), legend=None)\n    ).properties(width=305, height=200)\n\nalt.hconcat(\n    map_('tealblues'), map_('viridis'), map_('blueorange'),\n    data=alt.topo_feature(usa, 'counties')\n).transform_lookup(\n    lookup='id', from_=alt.LookupData(data=unemp, key='id', fields=['rate'])\n).configure_view(\n    stroke=None\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\nWhich color schemes do you find to be more effective? Why might that be? Modify the maps above to use other available schemes, as described in the Vega Color Schemes documentation.\n\n\n\nNow that we have some experience creating maps, let’s take a closer look at cartographic projections. As explained by Wikipedia,\n\nAll map projections necessarily distort the surface in some fashion. Depending on the purpose of the map, some distortions are acceptable and others are not; therefore, different map projections exist in order to preserve some properties of the sphere-like body at the expense of other properties.\n\nSome of the properties we might wish to consider include:\n\nArea: Does the projection distort region sizes?\nBearing: Does a straight line correspond to a constant direction of travel?\nDistance: Do lines of equal length correspond to equal distances on the globe?\nShape: Does the projection preserve spatial relations (angles) between points?\n\nSelecting an appropriate projection thus depends on the use case for the map. For example, if we are assessing land use and the extent of land matters, we might choose an area-preserving projection. If we want to visualize shockwaves emanating from an earthquake, we might focus the map on the quake’s epicenter and preserve distances outward from that point. Or, if we wish to aid navigation, the preservation of bearing and shape may be more important.\nWe can also characterize projections in terms of the projection surface. Cylindrical projections, for example, project surface points of the sphere onto a surrounding cylinder; the “unrolled” cylinder then provides our map. As we further describe below, we might alternatively project onto the surface of a cone (conic projections) or directly onto a flat plane (azimuthal projections).\nLet’s first build up our intuition by interacting with a variety of projections! Open the online Vega-Lite Cartographic Projections notebook. Use the controls on that page to select a projection and explore projection parameters, such as the scale (zooming) and x/y translation (panning). The rotation (yaw, pitch, roll) controls determine the orientation of the globe relative to the surface being projected upon.\n\n\nCylindrical projections map the sphere onto a surrounding cylinder, then unroll the cylinder. If the major axis of the cylinder is oriented north-south, meridians are mapped to straight lines. Pseudo-cylindrical projections represent a central meridian as a straight line, with other meridians “bending” away from the center.\n\nminimap = map.properties(width=225, height=225)\nalt.hconcat(\n    minimap.project(type='equirectangular').properties(title='equirectangular'),\n    minimap.project(type='mercator').properties(title='mercator'),\n    minimap.project(type='transverseMercator').properties(title='transverseMercator'),\n    minimap.project(type='naturalEarth1').properties(title='naturalEarth1')\n).properties(spacing=10).configure_view(stroke=None)\n\n\n\n\n\n\n\nEquirectangular (equirectangular): Scale lat, lon coordinate values directly.\nMercator (mercator): Project onto a cylinder, using lon directly, but subjecting lat to a non-linear transformation. Straight lines preserve constant compass bearings (rhumb lines), making this projection well-suited to navigation. However, areas in the far north or south can be greatly distorted.\nTransverse Mercator (transverseMercator): A mercator projection, but with the bounding cylinder rotated to a transverse axis. Whereas the standard Mercator projection has highest accuracy along the equator, the Transverse Mercator projection is most accurate along the central meridian.\nNatural Earth (naturalEarth1): A pseudo-cylindrical projection designed for showing the whole Earth in one view. \n\nConic projections map the sphere onto a cone, and then unroll the cone on to the plane. Conic projections are configured by two standard parallels, which determine where the cone intersects the globe.\n\nminimap = map.properties(width=180, height=130)\nalt.hconcat(\n    minimap.project(type='conicEqualArea').properties(title='conicEqualArea'),\n    minimap.project(type='conicEquidistant').properties(title='conicEquidistant'),\n    minimap.project(type='conicConformal', scale=35, translate=[90,65]).properties(title='conicConformal'),\n    minimap.project(type='albers').properties(title='albers'),\n    minimap.project(type='albersUsa').properties(title='albersUsa')\n).properties(spacing=10).configure_view(stroke=None)\n\n\n\n\n\n\n\nConic Equal Area (conicEqualArea): Area-preserving conic projection. Shape and distance are not preserved, but roughly accurate within standard parallels.\nConic Equidistant (conicEquidistant): Conic projection that preserves distance along the meridians and standard parallels.\nConic Conformal (conicConformal): Conic projection that preserves shape (local angles), but not area or distance.\nAlbers (albers): A variant of the conic equal area projection with standard parallels optimized for creating maps of the United States.\nAlbers USA (albersUsa): A hybrid projection for the 50 states of the United States of America. This projection stitches together three Albers projections with different parameters for the continental U.S., Alaska, and Hawaii. \n\nAzimuthal projections map the sphere directly onto a plane.\n\nminimap = map.properties(width=180, height=180)\nalt.hconcat(\n    minimap.project(type='azimuthalEqualArea').properties(title='azimuthalEqualArea'),\n    minimap.project(type='azimuthalEquidistant').properties(title='azimuthalEquidistant'),\n    minimap.project(type='orthographic').properties(title='orthographic'),\n    minimap.project(type='stereographic').properties(title='stereographic'),\n    minimap.project(type='gnomonic').properties(title='gnomonic')\n).properties(spacing=10).configure_view(stroke=None)\n\n\n\n\n\n\n\nAzimuthal Equal Area (azimuthalEqualArea): Accurately projects area in all parts of the globe, but does not preserve shape (local angles).\nAzimuthal Equidistant (azimuthalEquidistant): Preserves proportional distance from the projection center to all other points on the globe.\nOrthographic (orthographic): Projects a visible hemisphere onto a distant plane. Approximately matches a view of the Earth from outer space.\nStereographic (stereographic): Preserves shape, but not area or distance.\nGnomonic (gnomonic): Projects the surface of the sphere directly onto a tangent plane. Great circles around the Earth are projected to straight lines, showing the shortest path between points. \n\n\n\n\n\nThe examples above all draw from the vega-datasets collection, including geometric (TopoJSON) and tabular (airports, unemployment rates) data. A common challenge to getting starting with geographic visualization is collecting the necessary data for your task. A number of data providers abound, including services such as the United States Geological Survey and U.S. Census Bureau.\nIn many cases you may have existing data with a geographic component, but require additional measures or geometry. To help you get started, here is one workflow:\n\nVisit Natural Earth Data and browse to select data for regions and resolutions of interest. Download the corresponding zip file(s).\nGo to MapShaper and drop your downloaded zip file onto the page. Revise the data as desired, and then “Export” generated TopoJSON or GeoJSON files.\nLoad the exported data from MapShaper for use with Altair!\n\nOf course, many other tools – both open-source and proprietary – exist for working with geographic data. For more about geo-data wrangling and map creation, see Mike Bostock’s tutorial series on Command-Line Cartography.\n\n\n\nAt this point, we’ve only dipped our toes into the waters of map-making. (You didn’t expect a single notebook to impart centuries of learning, did you?) For example, we left untouched topics such as cartograms and conveying topography — as in Imhof’s illuminating book Cartographic Relief Presentation. Nevertheless, you should now be well-equipped to create a rich array of geo-visualizations. For more, MacEachren’s book How Maps Work: Representation, Visualization, and Design provides a valuable overview of map-making from the perspective of data visualization."
  },
  {
    "objectID": "docs/altair_data_transformation.html",
    "href": "docs/altair_data_transformation.html",
    "title": "Data visualization course",
    "section": "",
    "text": "In previous notebooks we learned how to use marks and visual encodings to represent individual data records. Here we will explore methods for transforming data, including the use of aggregates to summarize multiple records. Data transformation is an integral part of visualization: choosing the variables to show and their level of detail is just as important as choosing appropriate visual encodings. After all, it doesn’t matter how well chosen your visual encodings are if you are showing the wrong information!\nAs you work through this module, we recommend that you open the Altair Data Transformations documentation in another tab. It will be a useful resource if at any point you’d like more details or want to see what other transformations are available.\nThis notebook is part of the data visualization curriculum.\n\nimport pandas as pd\nimport altair as alt\n\n\n\nWe will be working with a table of data about motion pictures, taken from the vega-datasets collection. The data includes variables such as the film name, director, genre, release date, ratings, and gross revenues. However, be careful when working with this data: the films are from unevenly sampled years, using data combined from multiple sources. If you dig in you will find issues with missing values and even some subtle errors! Nevertheless, the data should prove interesting to explore…\nLet’s retrieve the URL for the JSON data file from the vega_datasets package, and then read the data into a Pandas data frame so that we can inspect its contents.\n\nmovies_url = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'\nmovies = pd.read_json(movies_url)\n\nHow many rows (records) and columns (fields) are in the movies dataset?\n\nmovies.shape\n\n(3201, 16)\n\n\nNow let’s peek at the first 5 rows of the table to get a sense of the fields and data types…\n\nmovies.head(5)\n\n\n\n\n\n  \n    \n      \n      Title\n      US_Gross\n      Worldwide_Gross\n      US_DVD_Sales\n      Production_Budget\n      Release_Date\n      MPAA_Rating\n      Running_Time_min\n      Distributor\n      Source\n      Major_Genre\n      Creative_Type\n      Director\n      Rotten_Tomatoes_Rating\n      IMDB_Rating\n      IMDB_Votes\n    \n  \n  \n    \n      0\n      The Land Girls\n      146083.0\n      146083.0\n      NaN\n      8000000.0\n      Jun 12 1998\n      R\n      NaN\n      Gramercy\n      None\n      None\n      None\n      None\n      NaN\n      6.1\n      1071.0\n    \n    \n      1\n      First Love, Last Rites\n      10876.0\n      10876.0\n      NaN\n      300000.0\n      Aug 07 1998\n      R\n      NaN\n      Strand\n      None\n      Drama\n      None\n      None\n      NaN\n      6.9\n      207.0\n    \n    \n      2\n      I Married a Strange Person\n      203134.0\n      203134.0\n      NaN\n      250000.0\n      Aug 28 1998\n      None\n      NaN\n      Lionsgate\n      None\n      Comedy\n      None\n      None\n      NaN\n      6.8\n      865.0\n    \n    \n      3\n      Let's Talk About Sex\n      373615.0\n      373615.0\n      NaN\n      300000.0\n      Sep 11 1998\n      None\n      NaN\n      Fine Line\n      None\n      Comedy\n      None\n      None\n      13.0\n      NaN\n      NaN\n    \n    \n      4\n      Slam\n      1009819.0\n      1087521.0\n      NaN\n      1000000.0\n      Oct 09 1998\n      R\n      NaN\n      Trimark\n      Original Screenplay\n      Drama\n      Contemporary Fiction\n      None\n      62.0\n      3.4\n      165.0\n    \n  \n\n\n\n\n\n\n\nWe’ll start our transformation tour by binning data into discrete groups and counting records to summarize those groups. The resulting plots are known as histograms.\nLet’s first look at unaggregated data: a scatter plot showing movie ratings from Rotten Tomatoes versus ratings from IMDB users. We’ll provide data to Altair by passing the movies data URL to the Chart method. (We could also pass the Pandas data frame directly to get the same result.) We can then encode the Rotten Tomatoes and IMDB ratings fields using the x and y channels:\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nTo summarize this data, we can bin a data field to group numeric values into discrete groups. Here we bin along the x-axis by adding bin=True to the x encoding channel. The result is a set of ten bins of equal step size, each corresponding to a span of ten ratings points.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=True),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nSetting bin=True uses default binning settings, but we can exercise more control if desired. Let’s instead set the maximum bin count (maxbins) to 20, which has the effect of doubling the number of bins. Now each bin corresponds to a span of five ratings points.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nWith the data binned, let’s now summarize the distribution of Rotten Tomatoes ratings. We will drop the IMDB ratings for now and instead use the y encoding channel to show an aggregate count of records, so that the vertical position of each point indicates the number of movies per Rotten Tomatoes rating bin.\nAs the count aggregate counts the number of total records in each bin regardless of the field values, we do not need to include a field name in the y encoding.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nTo arrive at a standard histogram, let’s change the mark type from circle to bar:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nWe can now examine the distribution of ratings more clearly: we can see fewer movies on the negative end, and a bit more movies on the high end, but a generally uniform distribution overall. Rotten Tomatoes ratings are determined by taking “thumbs up” and “thumbs down” judgments from film critics and calculating the percentage of positive reviews. It appears this approach does a good job of utilizing the full range of rating values.\nSimilarly, we can create a histogram for IMDB ratings by changing the field in the x encoding channel:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nIn contrast to the more uniform distribution we saw before, IMDB ratings exhibit a bell-shaped (though negatively skewed) distribution. IMDB ratings are formed by averaging scores (ranging from 1 to 10) provided by the site’s users. We can see that this form of measurement leads to a different shape than the Rotten Tomatoes ratings. We can also see that the mode of the distribution is between 6.5 and 7: people generally enjoy watching movies, potentially explaining the positive bias!\nNow let’s turn back to our scatter plot of Rotten Tomatoes and IMDB ratings. Here’s what happens if we bin both axes of our original plot.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n)\n\n\n\n\n\n\nDetail is lost due to overplotting, with many points drawn directly on top of each other.\nTo form a two-dimensional histogram we can add a count aggregate as before. As both the x and y encoding channels are already taken, we must use a different encoding channel to convey the counts. Here is the result of using circular area by adding a size encoding channel.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Size('count()')\n)\n\n\n\n\n\n\nAlternatively, we can encode counts using the color channel and change the mark type to bar. The result is a two-dimensional histogram in the form of a heatmap.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Color('count()')\n)\n\n\n\n\n\n\nCompare the size and color-based 2D histograms above. Which encoding do you think should be preferred? Why? In which plot can you more precisely compare the magnitude of individual values? In which plot can you more accurately see the overall density of ratings?\n\n\n\nCounts are just one type of aggregate. We might also calculate summaries using measures such as the average, median, min, or max. The Altair documentation includes the full set of available aggregation functions.\nLet’s look at some examples!\n\n\nDo different genres of films receive consistently different ratings from critics? As a first step towards answering this question, we might examine the average (a.k.a. the arithmetic mean) rating for each genre of movie.\nLet’s visualize genre along the y axis and plot average Rotten Tomatoes ratings along the x axis.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('average(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N')\n)\n\n\n\n\n\n\nThere does appear to be some interesting variation, but looking at the data as an alphabetical list is not very helpful for ranking critical reactions to the genres.\nFor a tidier picture, let’s sort the genres in descending order of average rating. To do so, we will add a sort parameter to the y encoding channel, stating that we wish to sort by the average (op, the aggregate operation) Rotten Tomatoes rating (the field) in descending order.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('average(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='average', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\nThe sorted plot suggests that critics think highly of documentaries, musicals, westerns, and dramas, but look down upon romantic comedies and horror films… and who doesn’t love null movies!?\n\n\n\nWhile averages are a common way to summarize data, they can sometimes mislead. For example, very large or very small values (outliers) might skew the average. To be safe, we can compare the genres according to the median ratings as well.\nThe median is a point that splits the data evenly, such that half of the values are less than the median and the other half are greater. The median is less sensitive to outliers and so is referred to as a robust statistic. For example, arbitrarily increasing the largest rating value will not cause the median to change.\nLet’s update our plot to use a median aggregate and sort by those values:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('median(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='median', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\nWe can see that some of the genres with similar averages have swapped places (films of unknown genre, or null, are now rated highest!), but the overall groups have stayed stable. Horror films continue to get little love from professional film critics.\nIt’s a good idea to stay skeptical when viewing aggregate statistics. So far we’ve only looked at point estimates. We have not examined how ratings vary within a genre.\nLet’s visualize the variation among the ratings to add some nuance to our rankings. Here we will encode the inter-quartile range (IQR) for each genre. The IQR is the range in which the middle half of data values reside. A quartile contains 25% of the data values. The inter-quartile range consists of the two middle quartiles, and so contains the middle 50%.\nTo visualize ranges, we can use the x and x2 encoding channels to indicate the starting and ending points. We use the aggregate functions q1 (the lower quartile boundary) and q3 (the upper quartile boundary) to provide the inter-quartile range. (In case you are wondering, q2 would be the median.)\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('q1(Rotten_Tomatoes_Rating):Q'),\n    alt.X2('q3(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='median', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\n\n\n\nNow let’s ask a completely different question: do box office returns vary by season?\nTo get an initial answer, let’s plot the median U.S. gross revenue by month.\nTo make this chart, use the timeUnit transform to map release dates to the month of the year. The result is similar to binning, but using meaningful time intervals. Other valid time units include year, quarter, date (numeric day in month), day (day of the week), and hours, as well as compound units such as yearmonth or hoursminutes. See the Altair documentation for a complete list of time units.\n\nalt.Chart(movies_url).mark_area().encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(US_Gross):Q')\n)\n\n\n\n\n\n\nLooking at the resulting plot, median movie sales in the U.S. appear to spike around the summer blockbuster season and the end of year holiday period. Of course, people around the world (not just the U.S.) go out to the movies. Does a similar pattern arise for worldwide gross revenue?\n\nalt.Chart(movies_url).mark_area().encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(Worldwide_Gross):Q')\n)\n\n\n\n\n\n\nYes!\n\n\n\n\nThe examples above all use transformations (bin, timeUnit, aggregate, sort) that are defined relative to an encoding channel. However, at times you may want to apply a chain of multiple transformations prior to visualization, or use transformations that don’t integrate into encoding definitions. For such cases, Altair and Vega-Lite support data transformations defined separately from encodings. These transformations are applied to the data before any encodings are considered.\nWe could also perform transformations using Pandas directly, and then visualize the result. However, using the built-in transforms allows our visualizations to be published more easily in other contexts; for example, exporting the Vega-Lite JSON to use in a stand-alone web interface. Let’s look at the built-in transforms supported by Altair, such as calculate, filter, aggregate, and window.\n\n\nThink back to our comparison of U.S. gross and worldwide gross. Doesn’t worldwide revenue include the U.S.? (Indeed it does.) How might we get a better sense of trends outside the U.S.?\nWith the calculate transform we can derive new fields. Here we want to subtract U.S. gross from worldwide gross. The calculate transform takes a Vega expression string to define a formula over a single record. Vega expressions use JavaScript syntax. The datum. prefix accesses a field value on the input record.\n\nalt.Chart(movies).mark_area().transform_calculate(\n    NonUS_Gross='datum.Worldwide_Gross - datum.US_Gross'\n).encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(NonUS_Gross):Q')\n)\n\n\n\n\n\n\n\nWe can see that seasonal trends hold outside the U.S., but with a more pronounced decline in the non-peak months.\n\n\n\nThe filter transform creates a new table with a subset of the original data, removing rows that fail to meet a provided predicate test. Similar to the calculate transform, filter predicates are expressed using the Vega expression language.\nBelow we add a filter to limit our initial scatter plot of IMDB vs. Rotten Tomatoes ratings to only films in the major genre of “Romantic Comedy”.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n).transform_filter('datum.Major_Genre == \"Romantic Comedy\"')\n\n\n\n\n\n\nHow does the plot change if we filter to view other genres? Edit the filter expression to find out.\nNow let’s filter to look at films released before 1970.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n).transform_filter('year(datum.Release_Date) < 1970')\n\n\n\n\n\n\nThey seem to score unusually high! Are older films simply better, or is there a selection bias towards more highly-rated older films in this dataset?\n\n\n\nWe have already seen aggregate transforms such as count and average in the context of encoding channels. We can also specify aggregates separately, as a pre-processing step for other transforms (as in the window transform examples below). The output of an aggregate transform is a new data table with records that contain both the groupby fields and the computed aggregate measures.\nLet’s recreate our plot of average ratings by genre, but this time using a separate aggregate transform. The output table from the aggregate transform contains 13 rows, one for each genre.\nTo order the y axis we must include a required aggregate operation in our sorting instructions. Here we use the max operator, which works fine because there is only one output record per genre. We could similarly use the min operator and end up with the same plot.\n\nalt.Chart(movies_url).mark_bar().transform_aggregate(\n    groupby=['Major_Genre'],\n    Average_Rating='average(Rotten_Tomatoes_Rating)'\n).encode(\n    alt.X('Average_Rating:Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='max', field='Average_Rating', order='descending'\n      )\n    )\n)\n\n\n\n\n\n\n\n\n\nThe window transform performs calculations over sorted groups of data records. Window transforms are quite powerful, supporting tasks such as ranking, lead/lag analysis, cumulative totals, and running sums or averages. Values calculated by a window transform are written back to the input data table as new fields. Window operations include the aggregate operations we’ve seen earlier, as well as specialized operations such as rank, row_number, lead, and lag. The Vega-Lite documentation lists all valid window operations.\nOne use case for a window transform is to calculate top-k lists. Let’s plot the top 20 directors in terms of total worldwide gross.\nWe first use a filter transform to remove records for which we don’t know the director. Otherwise, the director null would dominate the list! We then apply an aggregate to sum up the worldwide gross for all films, grouped by director. At this point we could plot a sorted bar chart, but we’d end up with hundreds and hundreds of directors. How can we limit the display to the top 20?\nThe window transform allows us to determine the top directors by calculating their rank order. Within our window transform definition we can sort by gross and use the rank operation to calculate rank scores according to that sort order. We can then add a subsequent filter transform to limit the data to only records with a rank value less than or equal to 20.\n\nalt.Chart(movies_url).mark_bar().transform_filter(\n    'datum.Director != null'\n).transform_aggregate(\n    Gross='sum(Worldwide_Gross)',\n    groupby=['Director']\n).transform_window(\n    Rank='rank()',\n    sort=[alt.SortField('Gross', order='descending')]\n).transform_filter(\n    'datum.Rank < 20'\n).encode(\n    alt.X('Gross:Q'),\n    alt.Y('Director:N', sort=alt.EncodingSortField(\n        op='max', field='Gross', order='descending'\n    ))\n)\n\n\n\n\n\n\nWe can see that Steven Spielberg has been quite successful in his career! However, showing sums might favor directors who have had longer careers, and so have made more movies and thus more money. What happens if we change the choice of aggregate operation? Who is the most successful director in terms of average or median gross per film? Modify the aggregate transform above!\nEarlier in this notebook we looked at histograms, which approximate the probability density function of a set of values. A complementary approach is to look at the cumulative distribution. For example, think of a histogram in which each bin includes not only its own count but also the counts from all previous bins — the result is a running total, with the last bin containing the total number of records. A cumulative chart directly shows us, for a given reference value, how many data values are less than or equal to that reference.\nAs a concrete example, let’s look at the cumulative distribution of films by running time (in minutes). Only a subset of records actually include running time information, so we first filter down to the subset of films for which we have running times. Next, we apply an aggregate to count the number of films per duration (implicitly using “bins” of 1 minute each). We then use a window transform to compute a running total of counts across bins, sorted by increasing running time.\n\nalt.Chart(movies_url).mark_line(interpolate='step-before').transform_filter(\n    'datum.Running_Time_min != null'\n).transform_aggregate(\n    groupby=['Running_Time_min'],\n    Count='count()',\n).transform_window(\n    Cumulative_Sum='sum(Count)',\n    sort=[alt.SortField('Running_Time_min', order='ascending')]\n).encode(\n    alt.X('Running_Time_min:Q', axis=alt.Axis(title='Duration (min)')),\n    alt.Y('Cumulative_Sum:Q', axis=alt.Axis(title='Cumulative Count of Films'))\n)\n\n\n\n\n\n\nLet’s examine the cumulative distribution of film lengths. We can see that films under 110 minutes make up about half of all the films for which we have running times. We see a steady accumulation of films between 90 minutes and 2 hours, after which the distribution begins to taper off. Though rare, the dataset does contain multiple films more than 3 hours long!\n\n\n\n\nWe’ve only scratched the surface of what data transformations can do! For more details, including all the available transformations and their parameters, see the Altair data transformation documentation.\nSometimes you will need to perform significant data transformation to prepare your data prior to using visualization tools. To engage in data wrangling right here in Python, you can use the Pandas library."
  },
  {
    "objectID": "docs/altair_marks_encoding.html",
    "href": "docs/altair_marks_encoding.html",
    "title": "Data visualization course",
    "section": "",
    "text": "A visualization represents data using a collection of graphical marks (bars, lines, points, etc.). The attributes of a mark — such as its position, shape, size, or color — serve as channels through which we can encode underlying data values.\nWith a basic framework of data types, marks, and encoding channels, we can concisely create a wide variety of visualizations. In this notebook, we explore each of these elements and show how to use them to create custom statistical graphics.\nThis notebook is part of the data visualization curriculum.\n\nimport pandas as pd\nimport altair as alt\n\n\n\nWe will be visualizing global health and population data for a number of countries, over the time period of 1955 to 2005. The data was collected by the Gapminder Foundation and shared in Hans Rosling’s popular TED talk. If you haven’t seen the talk, we encourage you to watch it first!\nLet’s first load the dataset from the vega-datasets collection into a Pandas data frame.\n\nfrom vega_datasets import data as vega_data\ndata = vega_data.gapminder()\n\nHow big is the data?\n\ndata.shape\n\n(693, 6)\n\n\n693 rows and 6 columns! Let’s take a peek at the data content:\n\ndata.head(5)\n\n\n\n\n\n  \n    \n      \n      year\n      country\n      cluster\n      pop\n      life_expect\n      fertility\n    \n  \n  \n    \n      0\n      1955\n      Afghanistan\n      0\n      8891209\n      30.332\n      7.7\n    \n    \n      1\n      1960\n      Afghanistan\n      0\n      9829450\n      31.997\n      7.7\n    \n    \n      2\n      1965\n      Afghanistan\n      0\n      10997885\n      34.020\n      7.7\n    \n    \n      3\n      1970\n      Afghanistan\n      0\n      12430623\n      36.088\n      7.7\n    \n    \n      4\n      1975\n      Afghanistan\n      0\n      14132019\n      38.438\n      7.7\n    \n  \n\n\n\n\nFor each country and year (in 5-year intervals), we have measures of fertility in terms of the number of children per woman (fertility), life expectancy in years (life_expect), and total population (pop).\nWe also see a cluster field with an integer code. What might this represent? We’ll try and solve this mystery as we visualize the data!\nLet’s also create a smaller data frame, filtered down to values for the year 2000 only:\n\ndata2000 = data.loc[data['year'] == 2000]\n\n\ndata2000.head(5)\n\n\n\n\n\n  \n    \n      \n      year\n      country\n      cluster\n      pop\n      life_expect\n      fertility\n    \n  \n  \n    \n      9\n      2000\n      Afghanistan\n      0\n      23898198\n      42.129\n      7.4792\n    \n    \n      20\n      2000\n      Argentina\n      3\n      37497728\n      74.340\n      2.3500\n    \n    \n      31\n      2000\n      Aruba\n      3\n      69539\n      73.451\n      2.1240\n    \n    \n      42\n      2000\n      Australia\n      4\n      19164620\n      80.370\n      1.7560\n    \n    \n      53\n      2000\n      Austria\n      1\n      8113413\n      78.980\n      1.3820\n    \n  \n\n\n\n\n\n\n\nThe first ingredient in effective visualization is the input data. Data values can represent different forms of measurement. What kinds of comparisons do those measurements support? And what kinds of visual encodings then support those comparisons?\nWe will start by looking at the basic data types that Altair uses to inform visual encoding choices. These data types determine the kinds of comparisons we can make, and thereby guide our visualization design decisions.\n\n\nNominal data (also called categorical data) consist of category names.\nWith nominal data we can compare the equality of values: is value A the same or different than value B? (A = B), supporting statements like “A is equal to B” or “A is not equal to B”. In the dataset above, the country field is nominal.\nWhen visualizing nominal data we should readily be able to see if values are the same or different: position, color hue (blue, red, green, etc.), and shape can help. However, using a size channel to encode nominal data might mislead us, suggesting rank-order or magnitude differences among values that do not exist!\n\n\n\nOrdinal data consist of values that have a specific ordering.\nWith ordinal data we can compare the rank-ordering of values: does value A come before or after value B? (A < B), supporting statements like “A is less than B” or “A is greater than B”. In the dataset above, we can treat the year field as ordinal.\nWhen visualizing ordinal data, we should perceive a sense of rank-order. Position, size, or color value (brightness) might be appropriate, where as color hue (which is not perceptually ordered) would be less appropriate.\n\n\n\nWith quantitative data we can measure numerical differences among values. There are multiple sub-types of quantitative data:\nFor interval data we can measure the distance (interval) between points: what is the distance to value A from value B? (A - B), supporting statements such as “A is 12 units away from B”.\nFor ratio data the zero-point is meaningful and so we can also measure proportions or scale factors: value A is what proportion of value B? (A / B), supporting statements such as “A is 10% of B” or “B is 7 times larger than A”.\nIn the dataset above, year is a quantitative interval field (the value of year “zero” is subjective), whereas fertility and life_expect are quantitative ratio fields (zero is meaningful for calculating proportions). Vega-Lite represents quantitative data, but does not make a distinction between interval and ratio types.\nQuantitative values can be visualized using position, size, or color value, among other channels. An axis with a zero baseline is essential for proportional comparisons of ratio values, but can be safely omitted for interval comparisons.\n\n\n\nTemporal values measure time points or intervals. This type is a special case of quantitative values (timestamps) with rich semantics and conventions (i.e., the Gregorian calendar). The temporal type in Vega-Lite supports reasoning about time units (year, month, day, hour, etc.), and provides methods for requesting specific time intervals.\nExample temporal values include date strings such as “2019-01-04” and “Jan 04 2019”, as well as standardized date-times such as the ISO date-time format: “2019-01-04T17:50:35.643Z”.\nThere are no temporal values in our global development dataset above, as the year field is simply encoded as an integer. For more details about using temporal data in Altair, see the Times and Dates documentation.\n\n\n\nThese data types are not mutually exclusive, but rather form a hierarchy: ordinal data support nominal (equality) comparisons, while quantitative data support ordinal (rank-order) comparisons.\nMoreover, these data types do not provide a fixed categorization. Just because a data field is represented using a number doesn’t mean we have to treat it as a quantitative type! For example, we might interpret a set of ages (10 years old, 20 years old, etc) as nominal (underage or overage), ordinal (grouped by year), or quantitative (calculate average age).\nNow let’s examine how to visually encode these data types!\n\n\n\n\nAt the heart of Altair is the use of encodings that bind data fields (with a given data type) to available encoding channels of a chosen mark type. In this notebook we’ll examine the following encoding channels:\n\nx: Horizontal (x-axis) position of the mark.\ny: Vertical (y-axis) position of the mark.\nsize: Size of the mark. May correspond to area or length, depending on the mark type.\ncolor: Mark color, specified as a legal CSS color.\nopacity: Mark opacity, ranging from 0 (fully transparent) to 1 (fully opaque).\nshape: Plotting symbol shape for point marks.\ntooltip: Tooltip text to display upon mouse hover over the mark.\norder: Mark ordering, determines line/area point order and drawing order.\ncolumn: Facet the data into horizontally-aligned subplots.\nrow: Facet the data into vertically-aligned subplots.\n\nFor a complete list of available channels, see the Altair encoding documentation.\n\n\nThe x encoding channel sets a mark’s horizontal position (x-coordinate). In addition, default choices of axis and title are made automatically. In the chart below, the choice of a quantitative data type results in a continuous linear axis scale:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q')\n)\n\n\n\n\n\n\n\n\n\nThe y encoding channel sets a mark’s vertical position (y-coordinate). Here we’ve added the cluster field using an ordinal (O) data type. The result is a discrete axis that includes a sized band, with a default step size, for each unique value:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:O')\n)\n\n\n\n\n\n\nWhat happens to the chart above if you swap the O and Q field types?\nIf we instead add the life_expect field as a quantitative (Q) variable, the result is a scatter plot with linear scales for both axes:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q')\n)\n\n\n\n\n\n\nBy default, axes for linear quantitative scales include zero to ensure a proper baseline for comparing ratio-valued data. In some cases, however, a zero baseline may be meaningless or you may want to focus on interval comparisons. To disable automatic inclusion of zero, configure the scale mapping using the encoding scale attribute:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q', scale=alt.Scale(zero=False)),\n    alt.Y('life_expect:Q', scale=alt.Scale(zero=False))\n)\n\n\n\n\n\n\nNow the axis scales no longer include zero by default. Some padding still remains, as the axis domain end points are automatically snapped to nice numbers like multiples of 5 or 10.\nWhat happens if you also add nice=False to the scale attribute above?\n\n\n\nThe size encoding channel sets a mark’s size or extent. The meaning of the channel can vary based on the mark type. For point marks, the size channel maps to the pixel area of the plotting symbol, such that the diameter of the point matches the square root of the size value.\nLet’s augment our scatter plot by encoding population (pop) on the size channel. As a result, the chart now also includes a legend for interpreting the size values.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q')\n)\n\n\n\n\n\n\nIn some cases we might be unsatisfied with the default size range. To provide a customized span of sizes, set the range parameter of the scale attribute to an array indicating the smallest and largest sizes. Here we update the size encoding to range from 0 pixels (for zero values) to 1,000 pixels (for the maximum value in the scale domain):\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]))\n)\n\n\n\n\n\n\n\n\n\nThe color encoding channel sets a mark’s color. The style of color encoding is highly dependent on the data type: nominal data will default to a multi-hued qualitative color scheme, whereas ordinal and quantitative data will use perceptually ordered color gradients.\nHere, we encode the cluster field using the color channel and a nominal (N) data type, resulting in a distinct hue for each cluster value. Can you start to guess what the cluster field might indicate?\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nIf we prefer filled shapes, we can can pass a filled=True parameter to the mark_point method:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nBy default, Altair uses a bit of transparency to help combat over-plotting. We are free to further adjust the opacity, either by passing a default value to the mark_* method, or using a dedicated encoding channel.\nHere we demonstrate how to provide a constant value to an encoding channel instead of binding a data field:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5)\n)\n\n\n\n\n\n\n\n\n\nThe shape encoding channel sets the geometric shape used by point marks. Unlike the other channels we have seen so far, the shape channel can not be used by other mark types. The shape encoding channel should only be used with nominal data, as perceptual rank-order and magnitude comparisons are not supported.\nLet’s encode the cluster field using shape as well as color. Using multiple channels for the same underlying data field is known as a redundant encoding. The resulting chart combines both color and shape information into a single symbol legend:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n\nBy this point, you might feel a bit frustrated: we’ve built up a chart, but we still don’t know what countries the visualized points correspond to! Let’s add interactive tooltips to enable exploration.\nThe tooltip encoding channel determines tooltip text to show when a user moves the mouse cursor over a mark. Let’s add a tooltip encoding for the country field, then investigate which countries are being represented.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country')\n)\n\n\n\n\n\n\nAs you mouse around you may notice that you can not select some of the points. For example, the largest dark blue circle corresponds to India, which is drawn on top of a country with a smaller population, preventing the mouse from hovering over that country. To fix this problem, we can use the order encoding channel.\nThe order encoding channel determines the order of data points, affecting both the order in which they are drawn and, for line and area marks, the order in which they are connected to one another.\nLet’s order the values in descending rank order by the population (pop), ensuring that smaller circles are drawn later than larger circles:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n)\n\n\n\n\n\n\nNow we can identify the smaller country being obscured by India: it’s Bangladesh!\nWe can also now figure out what the cluster field represents. Mouse over the various colored points to formulate your own explanation.\nAt this point we’ve added tooltips that show only a single property of the underlying data record. To show multiple values, we can provide the tooltip channel an array of encodings, one for each field we want to include:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Order('pop:Q', sort='descending'),\n    tooltip = [\n        alt.Tooltip('country:N'),\n        alt.Tooltip('fertility:Q'),\n        alt.Tooltip('life_expect:Q')\n    ]   \n)\n\n\n\n\n\n\nNow we can see multiple data fields upon mouse over!\n\n\n\nSpatial position is one of the most powerful and flexible channels for visual encoding, but what can we do if we already have assigned fields to the x and y channels? One valuable technique is to create a trellis plot, consisting of sub-plots that show a subset of the data. A trellis plot is one example of the more general technique of presenting data using small multiples of views.\nThe column and row encoding channels generate either a horizontal (columns) or vertical (rows) set of sub-plots, in which the data is partitioned according to the provided data field.\nHere is a trellis plot that divides the data into one column per `cluster` value:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n)\n\n\n\n\n\n\nThe plot above does not fit on screen, making it difficult to compare all the sub-plots to each other! We can set the default width and height properties to create a smaller set of multiples. Also, as the column headers already label the cluster values, let’s remove our color legend by setting it to None. To make better use of space we can also orient our size legend to the 'bottom' of the chart.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]),\n             legend=alt.Legend(orient='bottom', titleOrient='left')),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n).properties(width=135, height=135)\n\n\n\n\n\n\nUnderneath the hood, the column and row encodings are translated into a new specification that uses the facet view composition operator. We will re-visit faceting in greater depth later on!\nIn the meantime, can you rewrite the chart above to facet into rows instead of columns?\n\n\n\nIn later modules, we’ll dive into interaction techniques for data exploration. Here is a sneak peak: binding a range slider to the year field to enable interactive scrubbing through each year of data. Don’t worry if the code below is a bit confusing at this point, as we will cover interaction in detail later.\nDrag the slider back and forth to see how the data values change over time!\n\nselect_year = alt.selection_single(\n    name='select', fields=['year'], init={'year': 1955},\n    bind=alt.binding_range(min=1955, max=2005, step=5)\n)\n\nalt.Chart(data).mark_point(filled=True).encode(\n    alt.X('fertility:Q', scale=alt.Scale(domain=[0,9])),\n    alt.Y('life_expect:Q', scale=alt.Scale(domain=[0,90])),\n    alt.Size('pop:Q', scale=alt.Scale(domain=[0, 1200000000], range=[0,1000])),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n).add_selection(select_year).transform_filter(select_year)\n\n\n\n\n\n\n\n\n\n\nOur exploration of encoding channels above exclusively uses point marks to visualize the data. However, the point mark type is only one of the many geometric shapes that can be used to visually represent data. Altair includes a number of built-in mark types, including:\n\nmark_area() - Filled areas defined by a top-line and a baseline.\nmark_bar() - Rectangular bars.\nmark_circle() - Scatter plot points as filled circles.\nmark_line() - Connected line segments.\nmark_point() - Scatter plot points with configurable shapes.\nmark_rect() - Filled rectangles, useful for heatmaps.\nmark_rule() - Vertical or horizontal lines spanning the axis.\nmark_square() - Scatter plot points as filled squares.\nmark_text() - Scatter plot points represented by text.\nmark_tick() - Vertical or horizontal tick marks.\n\nFor a complete list, and links to examples, see the Altair marks documentation. Next, we will step through a number of the most commonly used mark types for statistical graphics.\n\n\nThe point mark type conveys specific points, as in scatter plots and dot plots. In addition to x and y encoding channels (to specify 2D point positions), point marks can use color, size, and shape encodings to convey additional data fields.\nBelow is a dot plot of fertility, with the cluster field redundantly encoded using both the y and shape channels.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\nIn addition to encoding channels, marks can be stylized by providing values to the mark_*() methods.\nFor example: point marks are drawn with stroked outlines by default, but can be specified to use filled shapes instead. Similarly, you can set a default size to set the total pixel area of the point mark.\n\nalt.Chart(data2000).mark_point(filled=True, size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n\nThe circle mark type is a convenient shorthand for point marks drawn as filled circles.\n\nalt.Chart(data2000).mark_circle(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n\nThe square mark type is a convenient shorthand for point marks drawn as filled squares.\n\nalt.Chart(data2000).mark_square(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n\nThe tick mark type conveys a data point using a short line segment or “tick”. These are particularly useful for comparing values along a single dimension with minimal overlap. A dot plot drawn with tick marks is sometimes referred to as a strip plot.\n\nalt.Chart(data2000).mark_tick().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n\nThe `bar` mark type draws a rectangle with a position, width, and height.\nThe plot below is a simple bar chart of the population (`pop`) of each country.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('country:N'),\n    alt.Y('pop:Q')\n)\n\n\n\n\n\n\nThe bar width is set to a default size. We will discuss how to adjust the bar width later in this notebook. (A subsequent notebook will take a closer look at configuring axes, scales, and legends.)\nBars can also be stacked. Let’s change the x encoding to use the cluster field, and encode country using the color channel. We’ll also disable the legend (which would be very long with colors for all countries!) and use tooltips for the country name.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('cluster:N'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n)\n\n\n\n\n\n\nIn the chart above, the use of the color encoding channel causes Altair / Vega-Lite to automatically stack the bar marks. Otherwise, bars would be drawn on top of each other! Try adding the parameter stack=None to the y encoding channel to see what happens if we don’t apply stacking…\nThe examples above create bar charts from a zero-baseline, and the y channel only encodes the non-zero value (or height) of the bar. However, the bar mark also allows you to specify starting and ending points to convey ranges.\nThe chart below uses the x (starting point) and x2 (ending point) channels to show the range of life expectancies within each regional cluster. Below we use the min and max aggregation functions to determine the end points of the range; we will discuss aggregation in greater detail in the next notebook!\nAlternatively, you can use x and width to provide a starting point plus offset, such that x2 = x + width.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('min(life_expect):Q'),\n    alt.X2('max(life_expect):Q'),\n    alt.Y('cluster:N')\n)\n\n\n\n\n\n\n\n\n\nThe line mark type connects plotted points with line segments, for example so that a line’s slope conveys information about the rate of change.\nLet’s plot a line chart of fertility per country over the years, using the full, unfiltered global development data frame. We’ll again hide the legend and use tooltips instead.\n\nalt.Chart(data).mark_line().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nWe can see interesting variations per country, but overall trends for lower numbers of children per family over time. Also note that we set a custom width of 400 pixels. Try changing (or removing) the widths and see what happens!\nLet’s change some of the default mark parameters to customize the plot. We can set the strokeWidth to determine the thickness of the lines and the opacity to add some transparency. By default, the line mark uses straight line segments to connect data points. In some cases we might want to smooth the lines. We can adjust the interpolation used to connect data points by setting the interpolate mark parameter. Let’s use 'monotone' interpolation to provide smooth lines that are also guaranteed not to inadvertently generate “false” minimum or maximum values as a result of the interpolation.\n\nalt.Chart(data).mark_line(\n    strokeWidth=3,\n    opacity=0.5,\n    interpolate='monotone'\n).encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nThe line mark can also be used to create slope graphs, charts that highlight the change in value between two comparison points using line slopes.\nBelow let’s create a slope graph comparing the populations of each country at minimum and maximum years in our full dataset: 1955 and 2005. We first create a new Pandas data frame filtered to those years, then use Altair to create the slope graph.\nBy default, Altair places the years close together. To better space out the years along the x-axis, we can indicate the size (in pixels) of discrete steps along the width of our chart as indicated by the comment below. Try adjusting the width step value below and see how the chart changes in response.\n\ndataTime = data.loc[(data['year'] == 1955) | (data['year'] == 2005)]\n\nalt.Chart(dataTime).mark_line(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width={\"step\": 50} # adjust the step parameter\n)\n\n\n\n\n\n\n\n\n\nThe area mark type combines aspects of line and bar marks: it visualizes connections (slopes) among data points, but also shows a filled region, with one edge defaulting to a zero-valued baseline.\nThe chart below is an area chart of population over time for just the United States:\n\ndataUS = data.loc[data['country'] == 'United States']\n\nalt.Chart(dataUS).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to line marks, area marks support an interpolate parameter.\n\nalt.Chart(dataUS).mark_area(interpolate='monotone').encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to bar marks, area marks also support stacking. Here we create a new data frame with data for the three North American countries, then plot them using an area mark and a color encoding channel to stack by country.\n\ndataNA = data.loc[\n    (data['country'] == 'United States') |\n    (data['country'] == 'Canada') |\n    (data['country'] == 'Mexico')\n]\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nBy default, stacking is performed relative to a zero baseline. However, other stack options are available:\n\ncenter - to stack relative to a baseline in the center of the chart, creating a streamgraph visualization, and\nnormalize - to normalize the summed data at each stacking point to 100%, enabling percentage comparisons.\n\nBelow we adapt the chart by setting the y encoding stack attribute to center. What happens if you instead set it normalize?\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack='center'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nTo disable stacking altogether, set the stack attribute to None. We can also add opacity as a default mark parameter to ensure we see the overlapping areas!\n\nalt.Chart(dataNA).mark_area(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack=None),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nThe area mark type also supports data-driven baselines, with both the upper and lower series determined by data fields. As with bar marks, we can use the x and x2 (or y and y2) channels to provide end points for the area mark.\nThe chart below visualizes the range of minimum and maximum fertility, per year, for North American countries:\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('min(fertility):Q'),\n    alt.Y2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)\n\n\n\n\n\n\nWe can see a larger range of values in 1995, from just under 4 to just under 7. By 2005, both the overall fertility values and the variability have declined, centered around 2 children per familty.\nAll the area mark examples above use a vertically oriented area. However, Altair and Vega-Lite support horizontal areas as well. Let’s transpose the chart above, simply by swapping the x and y channels.\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.Y('year:O'),\n    alt.X('min(fertility):Q'),\n    alt.X2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)\n\n\n\n\n\n\n\n\n\n\nWe’ve completed our tour of data types, encoding channels, and graphical marks! You should now be well-equipped to further explore the space of encodings, mark types, and mark parameters. For a comprehensive reference – including features we’ve skipped over here! – see the Altair marks and encoding documentation.\nIn the next module, we will look at the use of data transformations to create charts that summarize data or visualize new derived fields. In a later module, we’ll examine how to further customize your charts by modifying scales, axes, and legends.\nInterested in learning more about visual encoding?\n\nBertin’s taxonomy of visual encodings from Sémiologie Graphique, as adapted by Mike Bostock.\n\nThe systematic study of marks, visual encodings, and backing data types was initiated by Jacques Bertin in his pioneering 1967 work Sémiologie Graphique (The Semiology of Graphics). The image above illustrates position, size, value (brightness), texture, color (hue), orientation, and shape channels, alongside Bertin’s recommendations for the data types they support.\nThe framework of data types, marks, and channels also guides automated visualization design tools, starting with Mackinlay’s APT (A Presentation Tool) in 1986 and continuing in more recent systems such as Voyager and Draco.\nThe identification of nominal, ordinal, interval, and ratio types dates at least as far back as S. S. Steven’s 1947 article On the theory of scales of measurement."
  },
  {
    "objectID": "docs/altair_scales_axes_legends.html",
    "href": "docs/altair_scales_axes_legends.html",
    "title": "Data visualization course",
    "section": "",
    "text": "Visual encoding – mapping data to visual variables such as position, size, shape, or color – is the beating heart of data visualization. The workhorse that actually performs this mapping is the scale: a function that takes a data value as input (the scale domain) and returns a visual value, such as a pixel position or RGB color, as output (the scale range). Of course, a visualization is useless if no one can figure out what it conveys! In addition to graphical marks, a chart needs reference elements, or guides, that allow readers to decode the graphic. Guides such as axes (which visualize scales with spatial ranges) and legends (which visualize scales with color, size, or shape ranges), are the unsung heroes of effective data visualization!\nIn this notebook, we will explore the options Altair provides to support customized designs of scale mappings, axes, and legends, using a running example about the effectiveness of antibiotic drugs.\nThis notebook is part of the data visualization curriculum.\n\nimport pandas as pd\nimport altair as alt\n\n\n\nAfter World War II, antibiotics were considered “wonder drugs”, as they were an easy remedy for what had been intractable ailments. To learn which drug worked most effectively for which bacterial infection, performance of the three most popular antibiotics on 16 bacteria were gathered.\nWe will be using an antibiotics dataset from the vega-datasets collection. In the examples below, we will pass the URL directly to Altair:\n\nantibiotics = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/burtin.json'\n\nWe can first load the data with Pandas to view the dataset in its entirety and get acquainted with the available fields:\n\npd.read_json(antibiotics)\n\n\n\n\n\n  \n    \n      \n      Bacteria\n      Penicillin\n      Streptomycin\n      Neomycin\n      Gram_Staining\n      Genus\n    \n  \n  \n    \n      0\n      Aerobacter aerogenes\n      870.000\n      1.00\n      1.600\n      negative\n      other\n    \n    \n      1\n      Bacillus anthracis\n      0.001\n      0.01\n      0.007\n      positive\n      other\n    \n    \n      2\n      Brucella abortus\n      1.000\n      2.00\n      0.020\n      negative\n      other\n    \n    \n      3\n      Diplococcus pneumoniae\n      0.005\n      11.00\n      10.000\n      positive\n      other\n    \n    \n      4\n      Escherichia coli\n      100.000\n      0.40\n      0.100\n      negative\n      other\n    \n    \n      5\n      Klebsiella pneumoniae\n      850.000\n      1.20\n      1.000\n      negative\n      other\n    \n    \n      6\n      Mycobacterium tuberculosis\n      800.000\n      5.00\n      2.000\n      negative\n      other\n    \n    \n      7\n      Proteus vulgaris\n      3.000\n      0.10\n      0.100\n      negative\n      other\n    \n    \n      8\n      Pseudomonas aeruginosa\n      850.000\n      2.00\n      0.400\n      negative\n      other\n    \n    \n      9\n      Salmonella (Eberthella) typhosa\n      1.000\n      0.40\n      0.008\n      negative\n      Salmonella\n    \n    \n      10\n      Salmonella schottmuelleri\n      10.000\n      0.80\n      0.090\n      negative\n      Salmonella\n    \n    \n      11\n      Staphylococcus albus\n      0.007\n      0.10\n      0.001\n      positive\n      Staphylococcus\n    \n    \n      12\n      Staphylococcus aureus\n      0.030\n      0.03\n      0.001\n      positive\n      Staphylococcus\n    \n    \n      13\n      Streptococcus fecalis\n      1.000\n      1.00\n      0.100\n      positive\n      Streptococcus\n    \n    \n      14\n      Streptococcus hemolyticus\n      0.001\n      14.00\n      10.000\n      positive\n      Streptococcus\n    \n    \n      15\n      Streptococcus viridans\n      0.005\n      10.00\n      40.000\n      positive\n      Streptococcus\n    \n  \n\n\n\n\nThe numeric values in the table indicate the minimum inhibitory concentration (MIC), a measure of the effectiveness of the antibiotic, which represents the concentration of antibiotic (in micrograms per milliliter) required to prevent growth in vitro. The reaction of the bacteria to a procedure called Gram staining is described by the nominal field Gram_Staining. Bacteria that turn dark blue or violet are Gram-positive. Otherwise, they are Gram-negative.\nAs we examine different visualizations of this dataset, ask yourself: What might we learn about the relative effectiveness of the antibiotics? What might we learn about the bacterial species based on their antibiotic response?\n\n\n\n\n\nLet’s start by looking at a simple dot plot of the MIC for Neomycin.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q')\n)\n\n\n\n\n\n\nWe can see that the MIC values span orders of magnitude: most points to cluster on the left, with a few large outliers to the right.\nBy default Altair uses a linear mapping between the domain values (MIC) and the range values (pixels). To get a better overview of the data, we can apply a different scale transformation.\nTo change the scale type, we’ll set the scale attribute, using the alt.Scale method and type parameter.\nHere’s the result of using a square root (sqrt) scale type. Distances in the pixel range now correspond to the square root of distances in the data domain.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          scale=alt.Scale(type='sqrt'))\n)\n\n\n\n\n\n\nThe points on the left are now better differentiated, but we still see some heavy skew.\nLet’s try using a logarithmic scale (log) instead:\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          scale=alt.Scale(type='log'))\n)\n\n\n\n\n\n\nNow the data is much more evenly distributed and we can see the very large differences in concentrations required for different bacteria.\nIn a standard linear scale, a visual (pixel) distance of 10 units might correspond to an addition of 10 units in the data domain. A logarithmic transform maps between multiplication and addition, such that log(u) + log(v) = log(u*v). As a result, in a logarithmic scale, a visual distance of 10 units instead corresponds to multiplication by 10 units in the data domain, assuming a base 10 logarithm. The log scale above defaults to using the logarithm base 10, but we can adjust this by providing a base parameter to the scale.\n\n\n\nLower dosages indicate higher effectiveness. However, some people may expect values that are “better” to be “up and to the right” within a chart. If we want to cater to this convention, we can reverse the axis to encode “effectiveness” as a reversed MIC scale.\nTo do this, we can set the encoding sort property to 'descending':\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'))\n)\n\n\n\n\n\n\nUnfortunately the axis is starting to get a bit confusing: we’re plotting data on a logarithmic scale, in the reverse direction, and without a clear indication of what our units are!\nLet’s add a more informative axis title: we’ll use the title property of the encoding to provide the desired title text:\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Neomycin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nMuch better!\nBy default, Altair places the x-axis along the bottom of the chart. To change these defaults, we can add an axis attribute with orient='top':\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          axis=alt.Axis(orient='top'),\n          title='Neomycin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nSimilarly, the y-axis defaults to a 'left' orientation, but can be set to 'right'.\n\n\n\nHow does neomycin compare to other antibiotics, such as streptomycin and penicillin?\nTo start answering this question, we can create scatter plots, adding a y-axis encoding for another antibiotic that mirrors the design of our x-axis for neomycin.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Streptomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Streptomycin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nWe can see that neomycin and streptomycin appear highly correlated, as the bacterial strains respond similarly to both antibiotics.\nLet’s move on and compare neomycin with penicillin:\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Penicillin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nNow we see a more differentiated response: some bacteria respond well to neomycin but not penicillin, and vice versa!\nWhile this plot is useful, we can make it better. The x and y axes use the same units, but have different extents (the chart width is larger than the height) and different domains (0.001 to 100 for the x-axis, and 0.001 to 1,000 for the y-axis).\nLet’s equalize the axes: we can add explicit width and height settings for the chart, and specify matching domains using the scale domain property.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          title='Penicillin MIC (μg/ml, reverse log scale)')\n).properties(width=250, height=250)\n\n\n\n\n\n\nThe resulting plot is more balanced, and less prone to subtle misinterpretations!\nHowever, the grid lines are now rather dense. If we want to remove grid lines altogether, we can add grid=False to the axis attribute. But what if we instead want to reduce the number of tick marks, for example only including grid lines for each order of magnitude?\nTo change the number of ticks, we can specify a target tickCount property for an Axis object. The tickCount is treated as a suggestion to Altair, to be considered alongside other aspects such as using nice, human-friendly intervals. We may not get exactly the number of tick marks we request, but we should get something close.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)')\n).properties(width=250, height=250)\n\n\n\n\n\n\nBy setting the tickCount to 5, we have the desired effect.\nOur scatter plot points feel a bit small. Let’s change the default size by setting the size property of the circle mark. This size value is the area of the mark in pixels.\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'), \n).properties(width=250, height=250)\n\n\n\n\n\n\nHere we’ve set the circle mark area to 80 pixels. Further adjust the value as you see fit!\n\n\n\n\n\n\nAbove we saw that neomycin is more effective for some bacteria, while penicillin is more effective for others. But how can we tell which antibiotic to use if we don’t know the specific species of bacteria? Gram staining serves as a diagnostic for discriminating classes of bacteria!\nLet’s encode Gram_Staining on the color channel as a nominal data type:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N')\n).properties(width=250, height=250)\n\n\n\n\n\n\nWe can see that Gram-positive bacteria seem most susceptible to penicillin, whereas neomycin is more effective for Gram-negative bacteria!\nThe color scheme above was automatically chosen to provide perceptually-distinguishable colors for nominal (equal or not equal) comparisons. However, we might wish to customize the colors used. In this case, Gram staining results in distinctive physical colorings: pink for Gram-negative, purple for Gram-positive.\nLet’s use those colors by specifying an explicit scale mapping from the data domain to the color range:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N',\n          scale=alt.Scale(domain=['negative', 'positive'], range=['hotpink', 'purple'])\n    )\n).properties(width=250, height=250)\n\n\n\n\n\n\nBy default legends are placed on the right side of the chart. Similar to axes, we can change the legend orientation using the orient parameter:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N',\n          scale=alt.Scale(domain=['negative', 'positive'], range=['hotpink', 'purple']),\n          legend=alt.Legend(orient='left')\n    )\n).properties(width=250, height=250)\n\n\n\n\n\n\nWe can also remove a legend entirely by specifying legend=None:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N',\n          scale=alt.Scale(domain=['negative', 'positive'], range=['hotpink', 'purple']),\n          legend=None\n    )\n).properties(width=250, height=250)\n\n\n\n\n\n\n\n\n\nSo far we’ve considered the effectiveness of antibiotics. Let’s turn around and ask a different question: what might antibiotic response teach us about the different species of bacteria?\nTo start, let’s encode Bacteria (a nominal data field) using the color channel:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:N')\n).properties(width=250, height=250)\n\n\n\n\n\n\nThe result is a bit of a mess! There are enough unique bacteria that Altair starts repeating colors from its default 10-color palette for nominal values.\nTo use custom colors, we can update the color encoding scale property. One option is to provide explicit scale domain and range values to indicate the precise color mappings per value, as we did above for Gram staining. Another option is to use an alternative color scheme. Altair includes a variety of built-in color schemes. For a complete list, see the Vega color scheme documentation.\nLet’s try switching to a built-in 20-color scheme, tableau20, and set that using the scale scheme property.\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:N',\n          scale=alt.Scale(scheme='tableau20'))\n).properties(width=250, height=250)\n\n\n\n\n\n\n\nWe now have a unique color for each bacteria, but the chart is still a mess. Among other issues, the encoding takes no account of bacteria that belong to the same genus. In the chart above, the two different Salmonella strains have very different hues (teal and pink), despite being biological cousins.\nTo try a different scheme, we can also change the data type from nominal to ordinal. The default ordinal scheme uses blue shades, ramping from light to dark:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:O')\n).properties(width=250, height=250)\n\n\n\n\n\n\nSome of those blue shades may be hard to distinguish.\nFor more differentiated colors, we can experiment with alternatives to the default blues color scheme. The viridis scheme ramps through both hue and luminance:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:O',\n          scale=alt.Scale(scheme='viridis'))\n).properties(width=250, height=250)\n\n\n\n\n\n\nBacteria from the same genus now have more similar colors than before, but the chart still remains confusing. There are many colors, they are hard to look up in the legend accurately, and two bacteria may have similar colors but different genus.\n\n\n\nLet’s try to color by genus instead of bacteria. To do so, we will add a calculate transform that splits up the bacteria name on space characters and takes the first word in the resulting array. We can then encode the resulting Genus field using the tableau20 color scheme.\n(Note that the antibiotics dataset includes a pre-calculated Genus field, but we will ignore it here in order to further explore Altair’s data transformations.)\n\nalt.Chart(antibiotics).mark_circle(size=80).transform_calculate(\n    Genus='split(datum.Bacteria, \" \")[0]'\n).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Genus:N',\n          scale=alt.Scale(scheme='tableau20'))\n).properties(width=250, height=250)\n\n\n\n\n\n\n\nHmm… While the data are better segregated by genus, this cacapohony of colors doesn’t seem particularly useful.\nIf we look at some of the previous charts carefully, we can see that only a handful of bacteria have a genus shared with another bacteria: Salmonella, Staphylococcus, and Streptococcus. To focus our comparison, we might add colors only for these repeated genus values.\nLet’s add another calculate transform that takes a genus name, keeps it if it is one of the repeated values, and otherwise uses the string \"Other\".\nIn addition, we can add custom color encodings using explicit domain and range arrays for the color encoding scale.\n\nalt.Chart(antibiotics).mark_circle(size=80).transform_calculate(\n  Split='split(datum.Bacteria, \" \")[0]'\n).transform_calculate(\n  Genus='indexof([\"Salmonella\", \"Staphylococcus\", \"Streptococcus\"], datum.Split) >= 0 ? datum.Split : \"Other\"'\n).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Genus:N',\n          scale=alt.Scale(\n            domain=['Salmonella', 'Staphylococcus', 'Streptococcus', 'Other'],\n            range=['rgb(76,120,168)', 'rgb(84,162,75)', 'rgb(228,87,86)', 'rgb(121,112,110)']\n          ))\n).properties(width=250, height=250)\n\n\n\n\n\n\nWe now have a much more revealing plot, made possible by customizations to the axes and legend. Take a moment to examine the plot above. Notice any surprising groupings?\nThe upper-left region has a cluster of red Streptococcus bacteria, but with a grey Other bacteria alongside them. Meanwhile, towards the middle-right we see another red Streptococcus placed far away from it’s “cousins”. Might we expect bacteria from the same genus (and thus presumably more genetically similar) to be grouped closer together?\nAs it so happens, the underlying dataset actually contains errors. The dataset reflects the species designations used in the early 1950s. However, the scientific consensus has since been overturned. That gray point in the upper-left? It’s now considered a Streptococcus! That red point towards the middle-right? It’s no longer considered a Streptococcus!\nOf course, on its own, this dataset doesn’t fully justify these reclassifications. Nevertheless, the data contain valuable biological clues that went overlooked for decades! Visualization, when used by an appropriately skilled and inquisitive viewer, can be a powerful tool for discovery.\nThis example also reinforces an important lesson: always be skeptical of your data!\n\n\n\nWe might also use the color channel to encode quantitative values. Though keep in mind that typically color is not as effective for conveying quantities as position or size encodings!\nHere is a basic heatmap of penicillin MIC values for each bacteria. We’ll use a rect mark and sort the bacteria by descending MIC values (from most to least resistant):\n\nalt.Chart(antibiotics).mark_rect().encode(\n    alt.Y('Bacteria:N',\n      sort=alt.EncodingSortField(field='Penicillin', op='max', order='descending')\n    ),\n    alt.Color('Penicillin:Q')\n)\n\n\n\n\n\n\nWe can further improve this chart by combining features we’ve seen thus far: a log-transformed scale, a change of axis orientation, a custom color scheme (plasma), tick count adjustment, and custom title text. We’ll also exercise configuration options to adjust the axis title placement and legend title alignment.\n\nalt.Chart(antibiotics).mark_rect().encode(\n    alt.Y('Bacteria:N',\n      sort=alt.EncodingSortField(field='Penicillin', op='max', order='descending'),\n      axis=alt.Axis(\n        orient='right',     # orient axis on right side of chart\n        titleX=7,           # set x-position to 7 pixels right of chart\n        titleY=-2,          # set y-position to 2 pixels above chart\n        titleAlign='left',  # use left-aligned text\n        titleAngle=0        # undo default title rotation\n      )\n    ),\n    alt.Color('Penicillin:Q',\n      scale=alt.Scale(type='log', scheme='plasma', nice=True),\n      legend=alt.Legend(titleOrient='right', tickCount=5),\n      title='Penicillin MIC (μg/ml)'\n    )\n)\n\n\n\n\n\n\nAlternatively, we can remove the axis title altogether, and use the top-level title property to add a title for the entire chart:\n\nalt.Chart(antibiotics, title='Penicillin Resistance of Bacterial Strains').mark_rect().encode(\n    alt.Y('Bacteria:N',\n      sort=alt.EncodingSortField(field='Penicillin', op='max', order='descending'),\n      axis=alt.Axis(orient='right', title=None)\n    ),\n    alt.Color('Penicillin:Q',\n      scale=alt.Scale(type='log', scheme='plasma', nice=True),\n      legend=alt.Legend(titleOrient='right', tickCount=5),\n      title='Penicillin MIC (μg/ml)'\n    )\n).configure_title(\n  anchor='start', # anchor and left-align title\n  offset=5        # set title offset from chart\n)\n\n\n\n\n\n\n\n\n\n\nIntegrating what we’ve learned across the notebooks so far about encodings, data transforms, and customization, you should now be prepared to make a wide variety of statistical graphics. Now you can put Altair into everyday use for exploring and communicating data!\nInterested in learning more about this topic?\n\nStart with the Altair Customizing Visualizations documentation.\nFor a complementary discussion of scale mappings, see “Introducing d3-scale”.\nFor a more in-depth exploration of all the ways axes and legends can be styled by the underlying Vega library (which powers Altair and Vega-Lite), see “A Guide to Guides: Axes & Legends in Vega”.\nFor a fascinating history of the antibiotics dataset, see Wainer & Lysen’s “That’s Funny…” in the American Scientist."
  },
  {
    "objectID": "docs/altair_view_composition.html",
    "href": "docs/altair_view_composition.html",
    "title": "Data visualization course",
    "section": "",
    "text": "When visualizing a number of different data fields, we might be tempted to use as many visual encoding channels as we can: x, y, color, size, shape, and so on. However, as the number of encoding channels increases, a chart can rapidly become cluttered and difficult to read. An alternative to “over-loading” a single chart is to instead compose multiple charts in a way that facilitates rapid comparisons.\nIn this notebook, we will examine a variety of operations for multi-view composition:\n\nlayer: place compatible charts directly on top of each other,\nfacet: partition data into multiple charts, organized in rows or columns,\nconcatenate: position arbitrary charts within a shared layout, and\nrepeat: take a base chart specification and apply it to multiple data fields.\n\nWe’ll then look at how these operations form a view composition algebra, in which the operations can be combined to build a variety of complex multi-view displays.\nThis notebook is part of the data visualization curriculum.\n\nimport pandas as pd\nimport altair as alt\n\n\n\nWe will be visualizing weather statistics for the U.S. cities of Seattle and New York. Let’s load the dataset and peek at the first and last 10 rows:\n\nweather = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/weather.csv'\n\n\ndf = pd.read_csv(weather)\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      location\n      date\n      precipitation\n      temp_max\n      temp_min\n      wind\n      weather\n    \n  \n  \n    \n      0\n      Seattle\n      2012-01-01\n      0.0\n      12.8\n      5.0\n      4.7\n      drizzle\n    \n    \n      1\n      Seattle\n      2012-01-02\n      10.9\n      10.6\n      2.8\n      4.5\n      rain\n    \n    \n      2\n      Seattle\n      2012-01-03\n      0.8\n      11.7\n      7.2\n      2.3\n      rain\n    \n    \n      3\n      Seattle\n      2012-01-04\n      20.3\n      12.2\n      5.6\n      4.7\n      rain\n    \n    \n      4\n      Seattle\n      2012-01-05\n      1.3\n      8.9\n      2.8\n      6.1\n      rain\n    \n    \n      5\n      Seattle\n      2012-01-06\n      2.5\n      4.4\n      2.2\n      2.2\n      rain\n    \n    \n      6\n      Seattle\n      2012-01-07\n      0.0\n      7.2\n      2.8\n      2.3\n      rain\n    \n    \n      7\n      Seattle\n      2012-01-08\n      0.0\n      10.0\n      2.8\n      2.0\n      sun\n    \n    \n      8\n      Seattle\n      2012-01-09\n      4.3\n      9.4\n      5.0\n      3.4\n      rain\n    \n    \n      9\n      Seattle\n      2012-01-10\n      1.0\n      6.1\n      0.6\n      3.4\n      rain\n    \n  \n\n\n\n\n\ndf.tail(10)\n\n\n\n\n\n  \n    \n      \n      location\n      date\n      precipitation\n      temp_max\n      temp_min\n      wind\n      weather\n    \n  \n  \n    \n      2912\n      New York\n      2015-12-22\n      4.8\n      15.6\n      11.1\n      3.8\n      fog\n    \n    \n      2913\n      New York\n      2015-12-23\n      29.5\n      17.2\n      8.9\n      4.5\n      fog\n    \n    \n      2914\n      New York\n      2015-12-24\n      0.5\n      20.6\n      13.9\n      4.9\n      fog\n    \n    \n      2915\n      New York\n      2015-12-25\n      2.5\n      17.8\n      11.1\n      0.9\n      fog\n    \n    \n      2916\n      New York\n      2015-12-26\n      0.3\n      15.6\n      9.4\n      4.8\n      drizzle\n    \n    \n      2917\n      New York\n      2015-12-27\n      2.0\n      17.2\n      8.9\n      5.5\n      fog\n    \n    \n      2918\n      New York\n      2015-12-28\n      1.3\n      8.9\n      1.7\n      6.3\n      snow\n    \n    \n      2919\n      New York\n      2015-12-29\n      16.8\n      9.4\n      1.1\n      5.3\n      fog\n    \n    \n      2920\n      New York\n      2015-12-30\n      9.4\n      10.6\n      5.0\n      3.0\n      fog\n    \n    \n      2921\n      New York\n      2015-12-31\n      1.5\n      11.1\n      6.1\n      5.5\n      fog\n    \n  \n\n\n\n\nWe will create multi-view displays to examine weather within and across the cities.\n\n\n\nOne of the most common ways of combining multiple charts is to layer marks on top of each other. If the underlying scale domains are compatible, we can merge them to form shared axes. If either of the x or y encodings is not compatible, we might instead create a dual-axis chart, which overlays marks using separate scales and axes.\n\n\nLet’s start by plotting the minimum and maximum average temperatures per month:\n\nalt.Chart(weather).mark_area().encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_max):Q'),\n  alt.Y2('average(temp_min):Q')\n)\n\n\n\n\n\n\nThe plot shows us temperature ranges for each month over the entirety of our data. However, this is pretty misleading as it aggregates the measurements for both Seattle and New York!\nLet’s subdivide the data by location using a color encoding, while also adjusting the mark opacity to accommodate overlapping areas:\n\nalt.Chart(weather).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_max):Q'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\n\n\n\n\n\nWe can see that Seattle is more temperate: warmer in the winter, and cooler in the summer.\nIn this case we’ve created a layered chart without any special features by simply subdividing the area marks by color. While the chart above shows us the temperature ranges, we might also want to emphasize the middle of the range.\nLet’s create a line chart showing the average temperature midpoint. We’ll use a calculate transform to compute the midpoints between the minimum and maximum daily temperatures:\n\nalt.Chart(weather).mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\n\n\n\n\n\nAside: note the use of +datum.temp_min within the calculate transform. As we are loading the data directly from a CSV file without any special parsing instructions, the temperature values may be internally represented as string values. Adding the + in front of the value forces it to be treated as a number.\nWe’d now like to combine these charts by layering the midpoint lines over the range areas. Using the syntax chart1 + chart2, we can specify that we want a new layered chart in which chart1 is the first layer and chart2 is a second layer drawn on top:\n\ntempMinMax = alt.Chart(weather).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_max):Q'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart(weather).mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\ntempMinMax + tempMid\n\n\n\n\n\n\nNow we have a multi-layer plot! However, the y-axis title (though informative) has become a bit long and unruly…\nLet’s customize our axes to clean up the plot. If we set a custom axis title within one of the layers, it will automatically be used as a shared axis title for all the layers:\n\ntempMinMax = alt.Chart(weather).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart(weather).mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\ntempMinMax + tempMid\n\n\n\n\n\n\nWhat happens if both layers have custom axis titles? Modify the code above to find out…\nAbove used the + operator, a convenient shorthand for Altair’s layer method. We can generate an identical layered chart using the layer method directly:\n\nalt.layer(tempMinMax, tempMid)\n\n\n\n\n\n\nNote that the order of inputs to a layer matters, as subsequent layers will be drawn on top of earlier layers. Try swapping the order of the charts in the cells above. What happens? (Hint: look closely at the color of the line marks.)\n\n\n\nSeattle has a reputation as a rainy city. Is that deserved?\nLet’s look at precipitation alongside temperature to learn more. First let’s create a base plot the shows average monthly precipitation in Seattle:\n\nalt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T', title=None),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\n\n\n\n\n\nTo facilitate comparison with the temperature data, let’s create a new layered chart. Here’s what happens if we try to layer the charts as we did earlier:\n\ntempMinMax = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q')\n)\n\nprecip = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\nalt.layer(tempMinMax, precip)\n\n\n\n\n\n\nThe precipitation values use a much smaller range of the y-axis then the temperatures!\nBy default, layered charts use a shared domain: the values for the x-axis or y-axis are combined across all the layers to determine a shared extent. This default behavior assumes that the layered values have the same units. However, this doesn’t hold up for this example, as we are combining temperature values (degrees Celsius) with precipitation values (inches)!\nIf we want to use different y-axis scales, we need to specify how we want Altair to resolve the data across layers. In this case, we want to resolve the y-axis scale domains to be independent rather than use a shared domain. The Chart object produced by a layer operator includes a resolve_scale method with which we can specify the desired resolution:\n\ntempMinMax = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q')\n)\n\nprecip = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\nalt.layer(tempMinMax, precip).resolve_scale(y='independent')\n\n\n\n\n\n\nWe can now see that autumn is the rainiest season in Seattle (peaking in November), complemented by dry summers.\nYou may have noticed some redundancy in our plot specifications above: both use the same dataset and the same filter to look at Seattle only. If you want, you can streamline the code a bit by providing the data and filter transform to the top-level layered chart. The individual layers will then inherit the data if they don’t have their own data definitions:\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q')\n)\n\nprecip = alt.Chart().mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\nalt.layer(tempMinMax, precip, data=weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).resolve_scale(y='independent')\n\n\n\n\n\n\nWhile dual-axis charts can be useful, they are often prone to misinterpretation, as the different units and axis scales may be incommensurate. As is feasible, you might consider transformations that map different data fields to shared units, for example showing quantiles or relative percentage change.\n\n\n\n\nFaceting involves subdividing a dataset into groups and creating a separate plot for each group. In earlier notebooks, we learned how to create faceted charts using the row and column encoding channels. We’ll first review those channels and then show how they are instances of the more general facet operator.\nLet’s start with a basic histogram of maximum temperature values in Seattle:\n\nalt.Chart(weather).mark_bar().transform_filter(\n  'datum.location == \"Seattle\"'\n).encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q')\n)\n\n\n\n\n\n\nHow does this temperature profile change based on the weather of a given day – that is, whether there was drizzle, fog, rain, snow, or sun?\nLet’s use the column encoding channel to facet the data by weather type. We can also use color as a redundant encoding, using a customized color range:\n\ncolors = alt.Scale(\n  domain=['drizzle', 'fog', 'rain', 'snow', 'sun'],\n  range=['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n)\n\nalt.Chart(weather).mark_bar().transform_filter(\n  'datum.location == \"Seattle\"'\n).encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q'),\n  alt.Color('weather:N', scale=colors),\n  alt.Column('weather:N')\n).properties(\n  width=150,\n  height=150\n)\n\n\n\n\n\n\nUnsurprisingly, those rare snow days center on the coldest temperatures, followed by rainy and foggy days. Sunny days are warmer and, despite Seattle stereotypes, are the most plentiful. Though as any Seattleite can tell you, the drizzle occasionally comes, no matter the temperature!\nIn addition to row and column encoding channels within a chart definition, we can take a basic chart definition and apply faceting using an explicit facet operator.\nLet’s recreate the chart above, but this time using facet. We start with the same basic histogram definition, but remove the data source, filter transform, and column channel. We can then invoke the facet method, passing in the data and specifying that we should facet into columns according to the weather field. The facet method accepts both row and column arguments. The two can be used together to create a 2D grid of faceted plots.\nFinally we include our filter transform, applying it to the top-level faceted chart. While we could apply the filter transform to the histogram definition as before, that is slightly less efficient. Rather than filter out “New York” values within each facet cell, applying the filter to the faceted chart lets Vega-Lite know that we can filter out those values up front, prior to the facet subdivision.\n\ncolors = alt.Scale(\n  domain=['drizzle', 'fog', 'rain', 'snow', 'sun'],\n  range=['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n)\n\nalt.Chart().mark_bar().encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q'),\n  alt.Color('weather:N', scale=colors)\n).properties(\n  width=150,\n  height=150\n).facet(\n  data=weather,\n  column='weather:N'\n).transform_filter(\n  'datum.location == \"Seattle\"'\n)\n\n\n\n\n\n\nGiven all the extra code above, why would we want to use an explicit facet operator? For basic charts, we should certainly use the column or row encoding channels if we can. However, using the facet operator explicitly is useful if we want to facet composed views, such as layered charts.\nLet’s revisit our layered temperature plots from earlier. Instead of plotting data for New York and Seattle in the same plot, let’s break them up into separate facets. The individual chart definitions are nearly the same as before: one area chart and one line chart. The only difference is that this time we won’t pass the data directly to the chart constructors; we’ll wait and pass it to the facet operator later. We can layer the charts much as before, then invoke facet on the layered chart object, passing in the data and specifying column facets based on the location field:\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature (°C)'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart().mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\nalt.layer(tempMinMax, tempMid).facet(\n  data=weather,\n  column='location:N'\n)\n\n\n\n\n\n\nThe faceted charts we have seen so far use the same axis scale domains across the facet cells. This default of using shared scales and axes helps aid accurate comparison of values. However, in some cases you may wish to scale each chart independently, for example if the range of values in the cells differs significantly.\nSimilar to layered charts, faceted charts also support resolving to independent scales or axes across plots. Let’s see what happens if we call the resolve_axis method to request independent y-axes:\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature (°C)'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart().mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\nalt.layer(tempMinMax, tempMid).facet(\n  data=weather,\n  column='location:N'\n).resolve_axis(y='independent')\n\n\n\n\n\n\nThe chart above looks largely unchanged, but the plot for Seattle now includes its own axis.\nWhat if we instead call resolve_scale to resolve the underlying scale domains?\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature (°C)'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart().mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\nalt.layer(tempMinMax, tempMid).facet(\n  data=weather,\n  column='location:N'\n).resolve_scale(y='independent')\n\n\n\n\n\n\nNow we see facet cells with different axis scale domains. In this case, using independent scales seems like a bad idea! The domains aren’t very different, and one might be fooled into thinking that New York and Seattle have similar maximum summer temperatures.\nTo borrow a cliché: just because you can do something, doesn’t mean you should…\n\n\n\nFaceting creates small multiple plots that show separate subdivisions of the data. However, we might wish to create a multi-view display with different views of the same dataset (not subsets) or views involving different datasets.\nAltair provides concatenation operators to combine arbitrary charts into a composed chart. The hconcat operator (shorthand | ) performs horizontal concatenation, while the vconcat operator (shorthand &) performs vertical concatenation.\nLet’s start with a basic line chart showing the average maximum temperature per month for both New York and Seattle, much like we’ve seen before:\n\nalt.Chart(weather).mark_line().encode(\n  alt.X('month(date):T', title=None),\n  alt.Y('average(temp_max):Q'),\n  color='location:N'\n)\n\n\n\n\n\n\nWhat if we want to compare not just temperature over time, but also precipitation and wind levels?\nLet’s create a concatenated chart consisting of three plots. We’ll start by defining a “base” chart definition that contains all the aspects that should be shared by our three plots. We can then modify this base chart to create customized variants, with different y-axis encodings for the temp_max, precipitation, and wind fields. We can then concatenate them using the pipe (|) shorthand operator:\n\nbase = alt.Chart(weather).mark_line().encode(\n  alt.X('month(date):T', title=None),\n  color='location:N'\n).properties(\n  width=240,\n  height=180\n)\n\ntemp = base.encode(alt.Y('average(temp_max):Q'))\nprecip = base.encode(alt.Y('average(precipitation):Q'))\nwind = base.encode(alt.Y('average(wind):Q'))\n\ntemp | precip | wind\n\n\n\n\n\n\nAlternatively, we could use the more explicit alt.hconcat() method in lieu of the pipe | operator. Try rewriting the code above to use hconcat instead.\nVertical concatenation works similarly to horizontal concatenation. Using the & operator (or alt.vconcat method), modify the code to use a vertical ordering instead of a horizontal ordering.\nFinally, note that horizontal and vertical concatenation can be combined. What happens if you write something like (temp | precip) & wind?\nAside: Note the importance of those parentheses… what happens if you remove them? Keep in mind that these overloaded operators are still subject to Python’s operator precendence rules, and so vertical concatenation with & will take precedence over horizontal concatenation with |!\nAs we will revisit later, concatenation operators let you combine any and all charts into a multi-view dashboard!\n\n\n\nThe concatenation operators above are quite general, allowing arbitrary charts to be composed. Nevertheless, the example above was still a bit verbose: we have three very similar charts, yet have to define them separately and then concatenate them.\nFor cases where only one or two variables are changing, the repeat operator provides a convenient shortcut for creating multiple charts. Given a template specification with some free variables, the repeat operator will then create a chart for each specified assignment to those variables.\nLet’s recreate our concatenation example above using the repeat operator. The only aspect that changes across charts is the choice of data field for the y encoding channel. To create a template specification, we can use the repeater variable alt.repeat('column') as our y-axis field. This code simply states that we want to use the variable assigned to the column repeater, which organizes repeated charts in a horizontal direction. (As the repeater provides the field name only, we have to specify the field data type separately as type='quantitative'.)\nWe then invoke the repeat method, passing in data field names for each column:\n\nalt.Chart(weather).mark_line().encode(\n  alt.X('month(date):T',title=None),\n  alt.Y(alt.repeat('column'), aggregate='average', type='quantitative'),\n  color='location:N'\n).properties(\n  width=240,\n  height=180\n).repeat(\n  column=['temp_max', 'precipitation', 'wind']\n)\n\n\n\n\n\n\nRepetition is supported for both columns and rows. What happens if you modify the code above to use row instead of column?\nWe can also use row and column repetition together! One common visualization for exploratory data analysis is the scatter plot matrix (or SPLOM). Given a collection of variables to inspect, a SPLOM provides a grid of all pairwise plots of those variables, allowing us to assess potential associations.\nLet’s use the repeat operator to create a SPLOM for the temp_max, precipitation, and wind fields. We first create our template specification, with repeater variables for both the x- and y-axis data fields. We then invoke repeat, passing in arrays of field names to use for both row and column. Altair will then generate the cross product (or, Cartesian product) to create the full space of repeated charts:\n\nalt.Chart().mark_point(filled=True, size=15, opacity=0.5).encode(\n  alt.X(alt.repeat('column'), type='quantitative'),\n  alt.Y(alt.repeat('row'), type='quantitative')\n).properties(\n  width=150,\n  height=150\n).repeat(\n  data=weather,\n  row=['temp_max', 'precipitation', 'wind'],\n  column=['wind', 'precipitation', 'temp_max']\n).transform_filter(\n  'datum.location == \"Seattle\"'\n)\n\n\n\n\n\n\nLooking at these plots, there does not appear to be a strong association between precipitation and wind, though we do see that extreme wind and precipitation events occur in similar temperature ranges (~5-15° C). However, this observation is not particularly surprising: if we revisit our histogram at the beginning of the facet section, we can plainly see that the days with maximum temperatures in the range of 5-15° C are the most commonly occurring.\nModify the code above to get a better understanding of chart repetition. Try adding another variable (temp_min) to the SPLOM. What happens if you rearrange the order of the field names in either the row or column parameters for the repeat operator?\nFinally, to really appreciate what the repeat operator provides, take a moment to imagine how you might recreate the SPLOM above using only hconcat and vconcat!\n\n\n\nTogether, the composition operators layer, facet, concat, and repeat form a view composition algebra: the various operators can be combined to construct a variety of multi-view visualizations.\nAs an example, let’s start with two basic charts: a histogram and a simple line (a single rule mark) showing a global average.\n\nbasic1 = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_bar().encode(\n  alt.X('month(date):O'),\n  alt.Y('average(temp_max):Q')\n)\n\nbasic2 = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_rule(stroke='firebrick').encode(\n  alt.Y('average(temp_max):Q')\n)\n\nbasic1 | basic2\n\n\n\n\n\n\nWe can then combine the two charts using a layer operator, and then repeat that layered chart to show histograms with overlaid averages for multiple fields:\n\nalt.layer(\n  alt.Chart().mark_bar().encode(\n    alt.X('month(date):O', title='Month'),\n    alt.Y(alt.repeat('column'), aggregate='average', type='quantitative')\n  ),\n  alt.Chart().mark_rule(stroke='firebrick').encode(\n    alt.Y(alt.repeat('column'), aggregate='average', type='quantitative')\n  )\n).properties(\n  width=200,\n  height=150\n).repeat(\n  data=weather,\n  column=['temp_max', 'precipitation', 'wind']\n).transform_filter(\n  'datum.location == \"Seattle\"'\n)\n\n\n\n\n\n\nFocusing only on the multi-view composition operators, the model for the visualization above is:\nrepeat(column=[...])\n|- layer\n   |- basic1\n   |- basic2\nNow let’s explore how we can apply all the operators within a final dashboard that provides an overview of Seattle weather. We’ll combine the SPLOM and faceted histogram displays from earlier sections with the repeated histograms above:\n\nsplom = alt.Chart().mark_point(filled=True, size=15, opacity=0.5).encode(\n  alt.X(alt.repeat('column'), type='quantitative'),\n  alt.Y(alt.repeat('row'), type='quantitative')\n).properties(\n  width=125,\n  height=125\n).repeat(\n  row=['temp_max', 'precipitation', 'wind'],\n  column=['wind', 'precipitation', 'temp_max']\n)\n\ndateHist = alt.layer(\n  alt.Chart().mark_bar().encode(\n    alt.X('month(date):O', title='Month'),\n    alt.Y(alt.repeat('row'), aggregate='average', type='quantitative')\n  ),\n  alt.Chart().mark_rule(stroke='firebrick').encode(\n    alt.Y(alt.repeat('row'), aggregate='average', type='quantitative')\n  )\n).properties(\n  width=175,\n  height=125\n).repeat(\n  row=['temp_max', 'precipitation', 'wind']\n)\n\ntempHist = alt.Chart(weather).mark_bar().encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q'),\n  alt.Color('weather:N', scale=alt.Scale(\n    domain=['drizzle', 'fog', 'rain', 'snow', 'sun'],\n    range=['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n  ))\n).properties(\n  width=115,\n  height=100\n).facet(\n  column='weather:N'\n)\n\nalt.vconcat(\n  alt.hconcat(splom, dateHist),\n  tempHist,\n  data=weather,\n  title='Seattle Weather Dashboard'\n).transform_filter(\n  'datum.location == \"Seattle\"'\n).resolve_legend(\n  color='independent'\n).configure_axis(\n  labelAngle=0\n)\n\n\n\n\n\n\nThe full composition model for this dashboard is:\nvconcat\n|- hconcat\n|  |- repeat(row=[...], column=[...])\n|  |  |- splom base chart\n|  |- repeat(row=[...])\n|     |- layer\n|        |- dateHist base chart 1\n|        |- dateHist base chart 2\n|- facet(column='weather')\n   |- tempHist base chart\nPhew! The dashboard also includes a few customizations to improve the layout:\n\nWe adjust chart width and height properties to assist alignment and ensure the full visualization fits on the screen.\nWe add resolve_legend(color='independent') to ensure the color legend is associated directly with the colored histograms by temperature. Otherwise, the legend will resolve to the dashboard as a whole.\nWe use configure_axis(labelAngle=0) to ensure that no axis labels are rotated. This helps to ensure proper alignment among the scatter plots in the SPLOM and the histograms by month on the right.\n\nTry removing or modifying any of these adjustments and see how the dashboard layout responds!\nThis dashboard can be reused to show data for other locations or from other datasets. Update the dashboard to show weather patterns for New York instead of Seattle.\n\n\n\nFor more details on multi-view composition, including control over sub-plot spacing and header labels, see the Altair Compound Charts documentation.\nNow that we’ve seen how to compose multiple views, we’re ready to put them into action. In addition to statically presenting data, multiple views can enable interactive multi-dimensional exploration. For example, using linked selections we can highlight points in one view to see corresponding values highlight in other views.\nIn the next notebook, we’ll examine how to author interactive selections for both individual plots and multi-view compositions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "The Zen of Python states that:\n\n“There should be one– and preferably only one –obvious way to do it.”\n\nThe Python visualization landscape shown above doesn’t quite live up to this mantra. While R users have a clear standard for interactive data visualization and dashboarding with ggplot2 and Shiny, Pythonista’s are spoiled for choice.\nUpon closer inspection, however, one could argue that the library that supports your thinking as a data scientist, is the best one to use in your data science project. So if we were to choose a library specifically for the purpose of Exploratory Data Analysis (EDA) and presenting the results from a data science project in an interactive document (dashboard, app), which one would it be? And why?\nThese questions are adressed in the following two lectures, which introduce Altair and the underlying grammar of interactive graphics from Vega and Vega-Lite.\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\n\n\nThe term Exploratory Data Analysis (EDA) is mostly attributed to Tukey (1977). In EDA, one’s aim is not to draw conclusions on predefined research questions (be it the construction of a model, the estimation of parameters, or the confirmation or rejection of a hypothesis); in fact, EDA in problem solving is often applied to data collected without well-defined hypotheses. In EDA, one screens the data for clues that could inspire ideas and hypotheses.\nRead the paper by de Mast & Kemper (2009) to understand what the purpose of EDA and how it fits in the context of a data science project.\n\n\n\n\nIntroduction to AltairVega-Lite: Grammar of Interactive Graphics"
  }
]